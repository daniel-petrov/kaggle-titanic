{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "ce82cd61-8d56-4e36-baa1-640e88bcb848",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a375d755-2615-415b-b33c-e9c89e5d8a2f",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "deeb3871-de24-4c3c-9d11-f4639fee2206",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass  \\\n",
       "PassengerId                     \n",
       "1                   0       3   \n",
       "2                   1       1   \n",
       "3                   1       3   \n",
       "4                   1       1   \n",
       "5                   0       3   \n",
       "\n",
       "                                                          Name     Sex   Age  \\\n",
       "PassengerId                                                                    \n",
       "1                                      Braund, Mr. Owen Harris    male  22.0   \n",
       "2            Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0   \n",
       "3                                       Heikkinen, Miss. Laina  female  26.0   \n",
       "4                 Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0   \n",
       "5                                     Allen, Mr. William Henry    male  35.0   \n",
       "\n",
       "             SibSp  Parch            Ticket     Fare Cabin Embarked  \n",
       "PassengerId                                                          \n",
       "1                1      0         A/5 21171   7.2500   NaN        S  \n",
       "2                1      0          PC 17599  71.2833   C85        C  \n",
       "3                0      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "4                1      0            113803  53.1000  C123        S  \n",
       "5                0      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 317,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('titanic/train.csv')\n",
    "df.set_index('PassengerId', inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "215da41a-c5ca-486c-bedc-f51cfabdfce7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['male'] = df['Sex'].apply(lambda x: 1 if x == 'male' else 0)\n",
    "df['embarked_S'] = df['Embarked'].apply(lambda x: 1 if x == 'S' else 0)\n",
    "df['embarked_C'] = df['Embarked'].apply(lambda x: 1 if x == 'C' else 0)\n",
    "df['embarked_Q'] = df['Embarked'].apply(lambda x: 1 if x == 'Q' else 0)\n",
    "\n",
    "df.drop(columns=['Sex', 'Embarked', 'Cabin', 'Ticket', 'Name'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "27d0e436-ed86-4f5b-b169-7ed4ecf12883",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>male</th>\n",
       "      <th>embarked_S</th>\n",
       "      <th>embarked_C</th>\n",
       "      <th>embarked_Q</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.4583</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>13.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2250</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8792</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>860</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2292</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>864</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>69.5500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>869</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9.5000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>879</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.8958</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>889</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>23.4500</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>177 rows × 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived  Pclass  Age  SibSp  Parch     Fare  male  embarked_S  \\\n",
       "PassengerId                                                                   \n",
       "6                   0       3  NaN      0      0   8.4583     1           0   \n",
       "18                  1       2  NaN      0      0  13.0000     1           1   \n",
       "20                  1       3  NaN      0      0   7.2250     0           0   \n",
       "27                  0       3  NaN      0      0   7.2250     1           0   \n",
       "29                  1       3  NaN      0      0   7.8792     0           0   \n",
       "...               ...     ...  ...    ...    ...      ...   ...         ...   \n",
       "860                 0       3  NaN      0      0   7.2292     1           0   \n",
       "864                 0       3  NaN      8      2  69.5500     0           1   \n",
       "869                 0       3  NaN      0      0   9.5000     1           1   \n",
       "879                 0       3  NaN      0      0   7.8958     1           1   \n",
       "889                 0       3  NaN      1      2  23.4500     0           1   \n",
       "\n",
       "             embarked_C  embarked_Q  \n",
       "PassengerId                          \n",
       "6                     0           1  \n",
       "18                    0           0  \n",
       "20                    1           0  \n",
       "27                    1           0  \n",
       "29                    0           1  \n",
       "...                 ...         ...  \n",
       "860                   1           0  \n",
       "864                   0           0  \n",
       "869                   0           0  \n",
       "879                   0           0  \n",
       "889                   0           0  \n",
       "\n",
       "[177 rows x 10 columns]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "e9690cfa-4226-47ad-b572-70519d38147b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Age_imputed'] = np.where(df['Age'].isnull(), 1, 0)\n",
    "df['Age'].fillna(df['Age'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "225c14ea-2d06-43b4-8ada-447ae7be0670",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>male</th>\n",
       "      <th>embarked_S</th>\n",
       "      <th>embarked_C</th>\n",
       "      <th>embarked_Q</th>\n",
       "      <th>Age_imputed</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Survived, Pclass, Age, SibSp, Parch, Fare, male, embarked_S, embarked_C, embarked_Q, Age_imputed]\n",
       "Index: []"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "303b9d96-9033-4e33-ae07-68d7f67bb8f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: ylabel='Frequency'>"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['Age'].plot.hist(bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "6f9653d4-ee01-4816-91a6-67c2a93e9783",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='Survived'>"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGrCAYAAADqwWxuAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8pXeV/AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAf3UlEQVR4nO3dfWyV9f3/8deR0kPpzZG2eg4dB8FZja7otDigcwJCi4w7hwYV5iRjCwwldECIHdtki2sZiS1b2Mh0VVCsNZvWkTiRskGxqSRQrQJT502Z7eyxUetpi91pLdfvj2XX73u49XDT8y59PpIr2bmuzznnfZkd+/Q657Qex3EcAQAAGHJRvAcAAAA4FoECAADMIVAAAIA5BAoAADCHQAEAAOYQKAAAwBwCBQAAmJMQ7wHOxNGjR/Xhhx8qNTVVHo8n3uMAAIAvwXEcdXR0KCsrSxdddOprJP0yUD788EMFg8F4jwEAAM5AU1OTRowYcco1/TJQUlNTJf33BNPS0uI8DQAA+DLa29sVDAbdn+On0i8D5X9v66SlpREoAAD0M1/m4xl8SBYAAJhDoAAAAHMIFAAAYA6BAgAAzCFQAACAOQQKAAAwh0ABAADmECgAAMAcAgUAAJhDoAAAAHMIFAAAYA6BAgAAzCFQAACAOQQKAAAwh0ABAADmJMR7AMRm1AMvxHsE9KHD62bEewQAiAuuoAAAAHMIFAAAYA6BAgAAzCFQAACAOQQKAAAwh0ABAADmECgAAMAcAgUAAJhDoAAAAHMIFAAAYA6BAgAAzCFQAACAOQQKAAAwh0ABAADmECgAAMAcAgUAAJhDoAAAAHMIFAAAYA6BAgAAzCFQAACAOQQKAAAwh0ABAADmECgAAMAcAgUAAJhDoAAAAHMIFAAAYA6BAgAAzIkpUNauXSuPxxO1BQIB97jjOFq7dq2ysrKUlJSkSZMm6dChQ1GPEYlEtGzZMmVmZio5OVmzZ89Wc3PzuTkbAABwQYj5CsrXvvY1tbS0uNuBAwfcY+vXr1dpaak2btyoffv2KRAIKD8/Xx0dHe6awsJCVVVVqbKyUrW1ters7NTMmTPV29t7bs4IAAD0ewkx3yEhIeqqyf84jqMNGzZozZo1mjt3riRpy5Yt8vv9qqio0OLFixUOh1VeXq4nn3xSU6dOlSRt3bpVwWBQO3fu1LRp0074nJFIRJFIxL3d3t4e69gAAKAfifkKyjvvvKOsrCyNHj1ad911l95//31JUmNjo0KhkAoKCty1Xq9XEydOVF1dnSSpvr5ePT09UWuysrKUk5PjrjmRkpIS+Xw+dwsGg7GODQAA+pGYAmXcuHF64okn9NJLL+nRRx9VKBRSXl6ePvnkE4VCIUmS3++Puo/f73ePhUIhJSYmatiwYSddcyJFRUUKh8Pu1tTUFMvYAACgn4npLZ7p06e7/3vMmDGaMGGCvvrVr2rLli0aP368JMnj8UTdx3Gc4/Yd63RrvF6vvF5vLKMCAIB+7Ky+ZpycnKwxY8bonXfecT+XcuyVkNbWVveqSiAQUHd3t9ra2k66BgAA4KwCJRKJ6M0339Tw4cM1evRoBQIBVVdXu8e7u7tVU1OjvLw8SVJubq4GDx4ctaalpUUHDx501wAAAMT0Fs+qVas0a9YsjRw5Uq2trXrooYfU3t6ue++9Vx6PR4WFhSouLlZ2drays7NVXFysoUOHav78+ZIkn8+nRYsWaeXKlcrIyFB6erpWrVqlMWPGuN/qAQAAiClQmpubdffdd+vjjz/WJZdcovHjx2vv3r267LLLJEmrV69WV1eXli5dqra2No0bN047duxQamqq+xhlZWVKSEjQvHnz1NXVpSlTpmjz5s0aNGjQuT0zAADQb3kcx3HiPUSs2tvb5fP5FA6HlZaWFu9x+tSoB16I9wjoQ4fXzYj3CABwzsTy85u/xQMAAMwhUAAAgDkECgAAMIdAAQAA5hAoAADAHAIFAACYQ6AAAABzCBQAAGAOgQIAAMwhUAAAgDkECgAAMIdAAQAA5hAoAADAHAIFAACYQ6AAAABzCBQAAGAOgQIAAMwhUAAAgDkECgAAMIdAAQAA5hAoAADAHAIFAACYQ6AAAABzCBQAAGAOgQIAAMwhUAAAgDkECgAAMIdAAQAA5hAoAADAHAIFAACYQ6AAAABzCBQAAGAOgQIAAMwhUAAAgDkECgAAMIdAAQAA5hAoAADAHAIFAACYQ6AAAABzCBQAAGAOgQIAAMwhUAAAgDkECgAAMIdAAQAA5hAoAADAHAIFAACYQ6AAAABzCBQAAGAOgQIAAMwhUAAAgDkECgAAMIdAAQAA5hAoAADAnLMKlJKSEnk8HhUWFrr7HMfR2rVrlZWVpaSkJE2aNEmHDh2Kul8kEtGyZcuUmZmp5ORkzZ49W83NzWczCgAAuICccaDs27dPjzzyiK699tqo/evXr1dpaak2btyoffv2KRAIKD8/Xx0dHe6awsJCVVVVqbKyUrW1ters7NTMmTPV29t75mcCAAAuGGcUKJ2dnVqwYIEeffRRDRs2zN3vOI42bNigNWvWaO7cucrJydGWLVv0+eefq6KiQpIUDodVXl6uhx9+WFOnTtX111+vrVu36sCBA9q5c+cJny8Siai9vT1qAwAAF64zCpT77rtPM2bM0NSpU6P2NzY2KhQKqaCgwN3n9Xo1ceJE1dXVSZLq6+vV09MTtSYrK0s5OTnummOVlJTI5/O5WzAYPJOxAQBAPxFzoFRWVurVV19VSUnJccdCoZAkye/3R+33+/3usVAopMTExKgrL8euOVZRUZHC4bC7NTU1xTo2AADoRxJiWdzU1KTly5drx44dGjJkyEnXeTyeqNuO4xy371inWuP1euX1emMZFQAA9GMxXUGpr69Xa2urcnNzlZCQoISEBNXU1Oi3v/2tEhIS3Csnx14JaW1tdY8FAgF1d3erra3tpGsAAMDAFlOgTJkyRQcOHFBDQ4O7jR07VgsWLFBDQ4Muv/xyBQIBVVdXu/fp7u5WTU2N8vLyJEm5ubkaPHhw1JqWlhYdPHjQXQMAAAa2mN7iSU1NVU5OTtS+5ORkZWRkuPsLCwtVXFys7OxsZWdnq7i4WEOHDtX8+fMlST6fT4sWLdLKlSuVkZGh9PR0rVq1SmPGjDnuQ7cAAGBgiilQvozVq1erq6tLS5cuVVtbm8aNG6cdO3YoNTXVXVNWVqaEhATNmzdPXV1dmjJlijZv3qxBgwad63EAAEA/5HEcx4n3ELFqb2+Xz+dTOBxWWlpavMfpU6MeeCHeI6APHV43I94jAMA5E8vPb/4WDwAAMIdAAQAA5hAoAADAHAIFAACYQ6AAAABzCBQAAGAOgQIAAMwhUAAAgDkECgAAMIdAAQAA5hAoAADAHAIFAACYQ6AAAABzCBQAAGAOgQIAAMwhUAAAgDkECgAAMIdAAQAA5hAoAADAHAIFAACYQ6AAAABzCBQAAGAOgQIAAMwhUAAAgDkECgAAMIdAAQAA5hAoAADAHAIFAACYQ6AAAABzCBQAAGBOQrwHAAD816gHXoj3COhDh9fNiPcIpnEFBQAAmEOgAAAAcwgUAABgDoECAADMIVAAAIA5BAoAADCHQAEAAOYQKAAAwBwCBQAAmEOgAAAAcwgUAABgDoECAADMIVAAAIA5BAoAADCHQAEAAOYQKAAAwBwCBQAAmEOgAAAAcwgUAABgDoECAADMIVAAAIA5BAoAADAnpkDZtGmTrr32WqWlpSktLU0TJkzQiy++6B53HEdr165VVlaWkpKSNGnSJB06dCjqMSKRiJYtW6bMzEwlJydr9uzZam5uPjdnAwAALggxBcqIESO0bt067d+/X/v379ctt9yiOXPmuBGyfv16lZaWauPGjdq3b58CgYDy8/PV0dHhPkZhYaGqqqpUWVmp2tpadXZ2aubMmert7T23ZwYAAPqtmAJl1qxZ+va3v60rr7xSV155pX71q18pJSVFe/fuleM42rBhg9asWaO5c+cqJydHW7Zs0eeff66KigpJUjgcVnl5uR5++GFNnTpV119/vbZu3aoDBw5o586dJ33eSCSi9vb2qA0AAFy4zvgzKL29vaqsrNSRI0c0YcIENTY2KhQKqaCgwF3j9Xo1ceJE1dXVSZLq6+vV09MTtSYrK0s5OTnumhMpKSmRz+dzt2AweKZjAwCAfiDmQDlw4IBSUlLk9Xq1ZMkSVVVV6ZprrlEoFJIk+f3+qPV+v989FgqFlJiYqGHDhp10zYkUFRUpHA67W1NTU6xjAwCAfiQh1jtcddVVamho0GeffaZnn31W9957r2pqatzjHo8nar3jOMftO9bp1ni9Xnm93lhHBQAA/VTMV1ASExN1xRVXaOzYsSopKdF1112n3/zmNwoEApJ03JWQ1tZW96pKIBBQd3e32traTroGAADgrH8PiuM4ikQiGj16tAKBgKqrq91j3d3dqqmpUV5eniQpNzdXgwcPjlrT0tKigwcPumsAAABieovnJz/5iaZPn65gMKiOjg5VVlZq9+7d2r59uzwejwoLC1VcXKzs7GxlZ2eruLhYQ4cO1fz58yVJPp9PixYt0sqVK5WRkaH09HStWrVKY8aM0dSpU8/LCQIAgP4npkD56KOPdM8996ilpUU+n0/XXnuttm/frvz8fEnS6tWr1dXVpaVLl6qtrU3jxo3Tjh07lJqa6j5GWVmZEhISNG/ePHV1dWnKlCnavHmzBg0adG7PDAAA9Fsex3GceA8Rq/b2dvl8PoXDYaWlpcV7nD416oEX4j0C+tDhdTPiPQL6EK/vgWUgvr5j+fnN3+IBAADmECgAAMAcAgUAAJhDoAAAAHMIFAAAYA6BAgAAzCFQAACAOQQKAAAwh0ABAADmECgAAMAcAgUAAJhDoAAAAHMIFAAAYA6BAgAAzCFQAACAOQQKAAAwh0ABAADmECgAAMAcAgUAAJhDoAAAAHMIFAAAYA6BAgAAzCFQAACAOQQKAAAwh0ABAADmECgAAMAcAgUAAJhDoAAAAHMIFAAAYA6BAgAAzCFQAACAOQQKAAAwh0ABAADmECgAAMAcAgUAAJhDoAAAAHMIFAAAYA6BAgAAzCFQAACAOQQKAAAwh0ABAADmECgAAMAcAgUAAJhDoAAAAHMIFAAAYA6BAgAAzCFQAACAOQQKAAAwh0ABAADmECgAAMAcAgUAAJhDoAAAAHMIFAAAYE5MgVJSUqIbb7xRqampuvTSS3Xbbbfp7bffjlrjOI7Wrl2rrKwsJSUladKkSTp06FDUmkgkomXLlikzM1PJycmaPXu2mpubz/5sAADABSGmQKmpqdF9992nvXv3qrq6Wl988YUKCgp05MgRd8369etVWlqqjRs3at++fQoEAsrPz1dHR4e7prCwUFVVVaqsrFRtba06Ozs1c+ZM9fb2nrszAwAA/VZCLIu3b98edfvxxx/XpZdeqvr6et18881yHEcbNmzQmjVrNHfuXEnSli1b5Pf7VVFRocWLFyscDqu8vFxPPvmkpk6dKknaunWrgsGgdu7cqWnTpp2jUwMAAP3VWX0GJRwOS5LS09MlSY2NjQqFQiooKHDXeL1eTZw4UXV1dZKk+vp69fT0RK3JyspSTk6Ou+ZYkUhE7e3tURsAALhwnXGgOI6jFStW6KabblJOTo4kKRQKSZL8fn/UWr/f7x4LhUJKTEzUsGHDTrrmWCUlJfL5fO4WDAbPdGwAANAPnHGg3H///XrjjTf09NNPH3fM4/FE3XYc57h9xzrVmqKiIoXDYXdramo607EBAEA/cEaBsmzZMm3btk27du3SiBEj3P2BQECSjrsS0tra6l5VCQQC6u7uVltb20nXHMvr9SotLS1qAwAAF66YAsVxHN1///167rnn9Pe//12jR4+OOj569GgFAgFVV1e7+7q7u1VTU6O8vDxJUm5urgYPHhy1pqWlRQcPHnTXAACAgS2mb/Hcd999qqio0F/+8helpqa6V0p8Pp+SkpLk8XhUWFio4uJiZWdnKzs7W8XFxRo6dKjmz5/vrl20aJFWrlypjIwMpaena9WqVRozZoz7rR4AADCwxRQomzZtkiRNmjQpav/jjz+uhQsXSpJWr16trq4uLV26VG1tbRo3bpx27Nih1NRUd31ZWZkSEhI0b948dXV1acqUKdq8ebMGDRp0dmcDAAAuCB7HcZx4DxGr9vZ2+Xw+hcPhAfd5lFEPvBDvEdCHDq+bEe8R0Id4fQ8sA/H1HcvPb/4WDwAAMIdAAQAA5hAoAADAHAIFAACYQ6AAAABzCBQAAGAOgQIAAMwhUAAAgDkECgAAMIdAAQAA5hAoAADAHAIFAACYQ6AAAABzCBQAAGAOgQIAAMwhUAAAgDkECgAAMIdAAQAA5hAoAADAHAIFAACYQ6AAAABzCBQAAGAOgQIAAMwhUAAAgDkECgAAMIdAAQAA5hAoAADAHAIFAACYQ6AAAABzCBQAAGAOgQIAAMwhUAAAgDkECgAAMIdAAQAA5hAoAADAHAIFAACYQ6AAAABzCBQAAGAOgQIAAMwhUAAAgDkECgAAMIdAAQAA5hAoAADAHAIFAACYQ6AAAABzCBQAAGAOgQIAAMwhUAAAgDkECgAAMIdAAQAA5hAoAADAHAIFAACYQ6AAAABzYg6UPXv2aNasWcrKypLH49Hzzz8fddxxHK1du1ZZWVlKSkrSpEmTdOjQoag1kUhEy5YtU2ZmppKTkzV79mw1Nzef1YkAAIALR8yBcuTIEV133XXauHHjCY+vX79epaWl2rhxo/bt26dAIKD8/Hx1dHS4awoLC1VVVaXKykrV1taqs7NTM2fOVG9v75mfCQAAuGAkxHqH6dOna/r06Sc85jiONmzYoDVr1mju3LmSpC1btsjv96uiokKLFy9WOBxWeXm5nnzySU2dOlWStHXrVgWDQe3cuVPTpk07i9MBAAAXgnP6GZTGxkaFQiEVFBS4+7xeryZOnKi6ujpJUn19vXp6eqLWZGVlKScnx11zrEgkovb29qgNAABcuM5poIRCIUmS3++P2u/3+91joVBIiYmJGjZs2EnXHKukpEQ+n8/dgsHguRwbAAAYc16+xePxeKJuO45z3L5jnWpNUVGRwuGwuzU1NZ2zWQEAgD3nNFACgYAkHXclpLW11b2qEggE1N3drba2tpOuOZbX61VaWlrUBgAALlznNFBGjx6tQCCg6upqd193d7dqamqUl5cnScrNzdXgwYOj1rS0tOjgwYPuGgAAMLDF/C2ezs5Ovfvuu+7txsZGNTQ0KD09XSNHjlRhYaGKi4uVnZ2t7OxsFRcXa+jQoZo/f74kyefzadGiRVq5cqUyMjKUnp6uVatWacyYMe63egAAwMAWc6Ds379fkydPdm+vWLFCknTvvfdq8+bNWr16tbq6urR06VK1tbVp3Lhx2rFjh1JTU937lJWVKSEhQfPmzVNXV5emTJmizZs3a9CgQefglAAAQH/ncRzHifcQsWpvb5fP51M4HB5wn0cZ9cAL8R4BfejwuhnxHgF9iNf3wDIQX9+x/Pzmb/EAAABzCBQAAGAOgQIAAMwhUAAAgDkECgAAMIdAAQAA5hAoAADAHAIFAACYQ6AAAABzCBQAAGAOgQIAAMwhUAAAgDkECgAAMIdAAQAA5hAoAADAHAIFAACYQ6AAAABzCBQAAGAOgQIAAMwhUAAAgDkECgAAMIdAAQAA5hAoAADAHAIFAACYQ6AAAABzCBQAAGAOgQIAAMwhUAAAgDkECgAAMIdAAQAA5hAoAADAHAIFAACYQ6AAAABzCBQAAGAOgQIAAMwhUAAAgDkECgAAMIdAAQAA5hAoAADAHAIFAACYQ6AAAABzCBQAAGAOgQIAAMwhUAAAgDkECgAAMIdAAQAA5hAoAADAHAIFAACYQ6AAAABzCBQAAGAOgQIAAMwhUAAAgDkECgAAMCeugfL73/9eo0eP1pAhQ5Sbm6uXX345nuMAAAAj4hYozzzzjAoLC7VmzRq99tpr+ta3vqXp06frgw8+iNdIAADAiLgFSmlpqRYtWqQf/OAHuvrqq7VhwwYFg0Ft2rQpXiMBAAAjEuLxpN3d3aqvr9cDDzwQtb+goEB1dXXHrY9EIopEIu7tcDgsSWpvbz+/gxp0NPJ5vEdAHxqI/x8fyHh9DywD8fX9v3N2HOe0a+MSKB9//LF6e3vl9/uj9vv9foVCoePWl5SU6Be/+MVx+4PB4HmbEbDAtyHeEwA4Xwby67ujo0M+n++Ua+ISKP/j8XiibjuOc9w+SSoqKtKKFSvc20ePHtWnn36qjIyME67HhaW9vV3BYFBNTU1KS0uL9zgAziFe3wOL4zjq6OhQVlbWadfGJVAyMzM1aNCg466WtLa2HndVRZK8Xq+8Xm/Uvosvvvh8jgiD0tLS+BcYcIHi9T1wnO7Kyf/E5UOyiYmJys3NVXV1ddT+6upq5eXlxWMkAABgSNze4lmxYoXuuecejR07VhMmTNAjjzyiDz74QEuWLInXSAAAwIi4Bcqdd96pTz75RL/85S/V0tKinJwc/fWvf9Vll10Wr5FglNfr1YMPPnjc23wA+j9e3zgZj/NlvusDAADQh/hbPAAAwBwCBQAAmEOgAAAAcwgUAABgDoECAADMieuvugdOpLm5WZs2bVJdXZ1CoZA8Ho/8fr/y8vK0ZMkS/gYTAAwAfM0YptTW1mr69OkKBoMqKCiQ3++X4zhqbW1VdXW1mpqa9OKLL+qb3/xmvEcFcB40NTXpwQcf1GOPPRbvURBnBApMufHGG3XTTTeprKzshMd//OMfq7a2Vvv27evjyQD0hddff1033HCDent74z0K4oxAgSlJSUlqaGjQVVdddcLjb731lq6//np1dXX18WQAzoVt27ad8vj777+vlStXEijgMyiwZfjw4aqrqztpoLzyyisaPnx4H08F4Fy57bbb5PF4dKr/NvZ4PH04EawiUGDKqlWrtGTJEtXX1ys/P19+v18ej0ehUEjV1dX64x//qA0bNsR7TABnaPjw4frd736n22677YTHGxoalJub27dDwSQCBaYsXbpUGRkZKisr0x/+8Af3Mu+gQYOUm5urJ554QvPmzYvzlADOVG5url599dWTBsrprq5g4OAzKDCrp6dHH3/8sSQpMzNTgwcPjvNEAM7Wyy+/rCNHjujWW2894fEjR45o//79mjhxYh9PBmsIFAAAYA6/SRYAAJhDoAAAAHMIFAAAYA6BAgAAzCFQAJi1e/dueTweffbZZ+f1eRYuXHjSr70CiA8CBcBptba2avHixRo5cqS8Xq8CgYCmTZumV1555bw+b15enlpaWuTz+c7r8wCwh1/UBuC0br/9dvX09GjLli26/PLL9dFHH+lvf/ubPv300zN6PMdx1Nvbq4SEU/8rKDExUYFA4IyeA0D/xhUUAKf02Wefqba2Vr/+9a81efJkXXbZZfrGN76hoqIizZgxQ4cPH5bH41FDQ0PUfTwej3bv3i3p/79V89JLL2ns2LHyer0qLy+Xx+PRW2+9FfV8paWlGjVqlBzHiXqLJxwOKykpSdu3b49a/9xzzyk5OVmdnZ2SpH//+9+68847NWzYMGVkZGjOnDk6fPiwu763t1crVqzQxRdfrIyMDK1evZrfXAoYRKAAOKWUlBSlpKTo+eefVyQSOavHWr16tUpKSvTmm2/qjjvuUG5urp566qmoNRUVFZo/f/5xfzDO5/NpxowZJ1w/Z84cpaSk6PPPP9fkyZOVkpKiPXv2qLa2VikpKbr11lvV3d0tSXr44Yf12GOPqby8XLW1tfr0009VVVV1VucF4DxwAOA0/vznPzvDhg1zhgwZ4uTl5TlFRUXO66+/7jiO4zQ2NjqSnNdee81d39bW5khydu3a5TiO4+zatcuR5Dz//PNRj1taWupcfvnl7u23337bkeQcOnQo6n5tbW2O4zjOc88956SkpDhHjhxxHMdxwuGwM2TIEOeFF15wHMdxysvLnauuuso5evSo+5iRSMRJSkpyXnrpJcdxHGf48OHOunXr3OM9PT3OiBEjnDlz5pz9PygA5wxXUACc1u23364PP/xQ27Zt07Rp07R7927dcMMN2rx5c0yPM3bs2Kjbd911l/71r39p7969kqSnnnpKX//613XNNdec8P4zZsxQQkKCtm3bJkl69tlnlZqaqoKCAklSfX293n33XaWmprpXftLT0/Wf//xH7733nsLhsFpaWjRhwgT3MRMSEo6bC0D8ESgAvpQhQ4YoPz9fP//5z1VXV6eFCxfqwQcf1EUX/fdfI87/+RxHT0/PCR8jOTk56vbw4cM1efJkVVRUSJKefvppffe73z3pDImJibrjjjvc9RUVFbrzzjvdD9sePXpUubm5amhoiNr++c9/av78+Wd+8gD6HIEC4Ixcc801OnLkiC655BJJUktLi3vs/35g9nQWLFigZ555Rq+88oree+893XXXXaddv337dh06dEi7du3SggUL3GM33HCD3nnnHV166aW64oorojafzyefz6fhw4e7V2wk6YsvvlB9ff2XnhdA3yBQAJzSJ598oltuuUVbt27VG2+8ocbGRv3pT3/S+vXrNWfOHCUlJWn8+PFat26d/vGPf2jPnj366U9/+qUff+7cuWpvb9ePfvQjTZ48WV/5yldOuX7ixIny+/1asGCBRo0apfHjx7vHFixYoMzMTM2ZM0cvv/yyGhsbVVNTo+XLl6u5uVmStHz5cq1bt05VVVV66623tHTp0vP+i+AAxI5AAXBKKSkpGjdunMrKynTzzTcrJydHP/vZz/TDH/5QGzdulCQ99thj6unp0dixY7V8+XI99NBDX/rx09LSNGvWLL3++utRV0NOxuPx6O677z7h+qFDh2rPnj0aOXKk5s6dq6uvvlrf//731dXVpbS0NEnSypUr9b3vfU8LFy7UhAkTlJqaqu985zsx/BMB0Bc8jsMvAAAAALZwBQUAAJhDoAAAAHMIFAAAYA6BAgAAzCFQAACAOQQKAAAwh0ABAADmECgAAMAcAgUAAJhDoAAAAHMIFAAAYM7/AzAvrCmaK/j4AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['Survived'].value_counts().plot(kind='bar')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "640a98e7-f23b-4d20-8624-f50908cb17d6",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "e41d688e-03d4-460b-b7d0-9297398f0b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "3dc558cd-ba58-4112-b54b-d29168ad8ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['Pclass', 'Age', 'Age_imputed', 'SibSp', 'Parch', 'Fare', 'male', 'embarked_S', 'embarked_C', 'embarked_Q']]\n",
    "y = df[['Survived']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "c0b28d5a-f069-4db6-b373-a3f1f59525f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gridsearch(estimator, param_grid: dict, X=X, y=y):\n",
    "    gs = GridSearchCV(\n",
    "        estimator=estimator,\n",
    "        param_grid=param_grid,\n",
    "        n_jobs=-1,\n",
    "        cv=5,\n",
    "        verbose=5,\n",
    "        return_train_score=True\n",
    "    )\n",
    "    gs.fit(X, y.values.ravel())\n",
    "    results = pd.DataFrame(gs.cv_results_).sort_values(by=['mean_test_score'], ascending=False)\n",
    "    print(f\"Best score: {results.iloc[0]['mean_test_score']}\")\n",
    "    print(f\"Fit time: {results.iloc[0]['mean_fit_time']}\")\n",
    "    print(f\"Best params: {results.iloc[0]['params']}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59dc3813-9a06-492d-99a6-ff47c4e5cbcc",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "55a81fc7-5a1a-4609-ae73-3bf9ad16f45a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 36 candidates, totalling 180 fits\n",
      "Best score: 0.7957880861213985\n",
      "Fit time: 0.0065342426300048825\n",
      "Best params: {'class_weight': None, 'criterion': 'log_loss', 'max_depth': 100, 'splitter': 'random'}\n"
     ]
    }
   ],
   "source": [
    "res = gridsearch(\n",
    "    estimator=DecisionTreeClassifier(),\n",
    "    param_grid={\n",
    "        'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'splitter': ['best', 'random'],\n",
    "        'max_depth': [100, 1000, None],\n",
    "        'class_weight': ['balanced', None]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7700596-f272-421b-8315-e2c01948c664",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "f6205911-5435-4405-a705-7b01dbdb64fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 18 candidates, totalling 90 fits\n",
      "Best score: 0.8204695248258114\n",
      "Fit time: 0.02642374038696289\n",
      "Best params: {'criterion': 'gini', 'max_depth': 1000, 'n_estimators': 10}\n"
     ]
    }
   ],
   "source": [
    "res = gridsearch(\n",
    "    estimator=RandomForestClassifier(),\n",
    "    param_grid={\n",
    "        'n_estimators': [10, 100, 1000],\n",
    "        'criterion': ['gini', 'entropy', 'log_loss'],\n",
    "        'max_depth': [1000, None]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5321f186-d0ef-433d-bdde-542532a53d2d",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "dfde9b4b-c0c3-4779-9df2-83c7ecf79693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 20 candidates, totalling 100 fits\n",
      "Best score: 0.786761659657272\n",
      "Fit time: 0.11688823699951172\n",
      "Best params: {'class_weight': None, 'max_iter': 10000, 'penalty': 'l2', 'solver': 'lbfgs'}\n"
     ]
    }
   ],
   "source": [
    "res = gridsearch(\n",
    "    estimator=LogisticRegression(),\n",
    "    param_grid={\n",
    "        'penalty': ['l2', None],\n",
    "        'class_weight': ['balanced', None],\n",
    "        'solver': ['lbfgs', 'newton-cg', 'newton-cholesky', 'sag', 'saga'],\n",
    "        'max_iter': [10000]\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3275ccc-5332-4cfc-90d8-89ed72436836",
   "metadata": {},
   "source": [
    "## Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "8cf7a5f9-32b0-42d9-89cf-696a9842a137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 30 candidates, totalling 150 fits\n",
      "Best score: 0.8260749482141737\n",
      "Fit time: 2.214206600189209\n",
      "Best params: {'learning_rate': 0.01, 'loss': 'log_loss', 'n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "res = gridsearch(\n",
    "    estimator=GradientBoostingClassifier(),\n",
    "    param_grid={\n",
    "        'loss': ['log_loss', 'exponential'],\n",
    "        'learning_rate': [0.001, 0.01, 0.1, 1, 10],\n",
    "        'n_estimators': [10, 100, 1000],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15ebda0-08c1-4759-a9eb-aed11f921443",
   "metadata": {},
   "source": [
    "# Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "cb3a6b5b-a85f-48c2-b4c5-8b4890d7d2b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingClassifier(\n",
    "    learning_rate=0.01, loss='log_loss', n_estimators=1000\n",
    ").fit(X, y.values.ravel())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "12a45f20-b07b-4cb0-b185-ec51a7e2d9f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Age_imputed</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>male</th>\n",
       "      <th>embarked_S</th>\n",
       "      <th>embarked_C</th>\n",
       "      <th>embarked_Q</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1044</th>\n",
       "      <td>3</td>\n",
       "      <td>60.5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             Pclass   Age  Age_imputed  SibSp  Parch  Fare  male  embarked_S  \\\n",
       "PassengerId                                                                    \n",
       "1044              3  60.5            0      0      0   NaN     1           1   \n",
       "\n",
       "             embarked_C  embarked_Q  \n",
       "PassengerId                          \n",
       "1044                  0           0  "
      ]
     },
     "execution_count": 306,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test = pd.read_csv('titanic/test.csv')\n",
    "df_test.set_index('PassengerId', inplace=True)\n",
    "\n",
    "df_test['male'] = df_test['Sex'].apply(lambda x: 1 if x == 'male' else 0)\n",
    "df_test['embarked_S'] = df_test['Embarked'].apply(lambda x: 1 if x == 'S' else 0)\n",
    "df_test['embarked_C'] = df_test['Embarked'].apply(lambda x: 1 if x == 'C' else 0)\n",
    "df_test['embarked_Q'] = df_test['Embarked'].apply(lambda x: 1 if x == 'Q' else 0)\n",
    "df_test.drop(columns=['Sex', 'Embarked', 'Cabin', 'Ticket', 'Name'], inplace=True)\n",
    "\n",
    "df_test['Age_imputed'] = np.where(df_test['Age'].isnull(), 1, 0)\n",
    "df_test['Age'].fillna(df_test['Age'].mean(), inplace=True)\n",
    "\n",
    "X_test = df_test[['Pclass', 'Age', 'Age_imputed', 'SibSp', 'Parch', 'Fare', 'male', 'embarked_S', 'embarked_C', 'embarked_Q']]\n",
    "X_test[X_test.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "49f3b0ca-57d0-4ba5-bd09-32393c008766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Age</th>\n",
       "      <th>Age_imputed</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Fare</th>\n",
       "      <th>male</th>\n",
       "      <th>embarked_S</th>\n",
       "      <th>embarked_C</th>\n",
       "      <th>embarked_Q</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Pclass, Age, Age_imputed, SibSp, Parch, Fare, male, embarked_S, embarked_C, embarked_Q]\n",
       "Index: []"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.loc[1044, 'Fare'] = X_test[\n",
    "    (X_test['Pclass'] == 3) & \n",
    "    (X_test['SibSp'] == 0) &\n",
    "    (X_test['Parch'] == 0) &\n",
    "    (X_test['male'] == 1) &\n",
    "    (X_test['embarked_S'] == 1)\n",
    "]['Fare'].mean()\n",
    "X_test[X_test.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "6750ed15-a9a2-4991-bd5c-e12bb9e9612a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 1,\n",
       "       1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 1,\n",
       "       1, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "       0, 1, 1, 1, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "       0, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1,\n",
       "       1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1,\n",
       "       0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0,\n",
       "       1, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1,\n",
       "       0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0,\n",
       "       1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 0,\n",
       "       1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1,\n",
       "       0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict(X_test)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "031b6249-5bdf-47d0-b10f-88ffc604225f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index([ 892,  893,  894,  895,  896,  897,  898,  899,  900,  901,\n",
       "       ...\n",
       "       1300, 1301, 1302, 1303, 1304, 1305, 1306, 1307, 1308, 1309],\n",
       "      dtype='int64', name='PassengerId', length=418)"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "e1878e71-9772-4a39-a67b-5145e447e81e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Survived</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>892</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>893</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>894</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>895</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>896</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1305</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1306</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1307</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1308</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1309</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>418 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             Survived\n",
       "PassengerId          \n",
       "892                 0\n",
       "893                 0\n",
       "894                 0\n",
       "895                 0\n",
       "896                 0\n",
       "...               ...\n",
       "1305                0\n",
       "1306                1\n",
       "1307                0\n",
       "1308                0\n",
       "1309                0\n",
       "\n",
       "[418 rows x 1 columns]"
      ]
     },
     "execution_count": 310,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = pd.DataFrame(index=X_test.index)\n",
    "result['Survived'] = pred\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "8bdf3abb-f6dd-4e81-9bec-e4d13cfff6be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 2/5] END class_weight=balanced, criterion=gini, max_depth=100, splitter=random;, score=(train=0.987, test=0.719) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=log_loss, max_depth=None, splitter=random;, score=(train=0.985, test=0.753) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=gini, max_depth=100, splitter=best;, score=(train=0.982, test=0.820) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=gini, max_depth=1000, splitter=best;, score=(train=0.985, test=0.737) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=gini, max_depth=1000, splitter=random;, score=(train=0.985, test=0.798) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=gini, max_depth=None, splitter=random;, score=(train=0.987, test=0.775) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=gini, max_depth=None, splitter=random;, score=(train=0.985, test=0.770) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=entropy, max_depth=1000, splitter=best;, score=(train=0.982, test=0.781) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=entropy, max_depth=1000, splitter=random;, score=(train=0.985, test=0.765) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=log_loss, max_depth=100, splitter=best;, score=(train=0.985, test=0.737) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=log_loss, max_depth=100, splitter=best;, score=(train=0.987, test=0.781) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=log_loss, max_depth=None, splitter=best;, score=(train=0.985, test=0.737) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=log_loss, max_depth=None, splitter=best;, score=(train=0.987, test=0.775) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=log_loss, max_depth=100, splitter=random;, score=(train=0.987, test=0.758) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=log_loss, max_depth=100, splitter=random;, score=(train=0.985, test=0.787) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=log_loss, max_depth=100, splitter=random;, score=(train=0.985, test=0.753) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=log_loss, max_depth=100, splitter=random;, score=(train=0.982, test=0.792) total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=1000, n_estimators=10;, score=(train=0.969, test=0.787) total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=None, n_estimators=100;, score=(train=0.985, test=0.848) total time=   0.3s\n",
      "[CV 5/5] END criterion=gini, max_depth=None, n_estimators=1000;, score=(train=0.982, test=0.854) total time=   3.8s\n",
      "[CV 2/5] END criterion=log_loss, max_depth=1000, n_estimators=100;, score=(train=0.987, test=0.792) total time=   0.4s\n",
      "[CV 4/5] END criterion=log_loss, max_depth=1000, n_estimators=1000;, score=(train=0.985, test=0.764) total time=   3.5s\n",
      "[CV 4/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=saga;, score=(train=0.788, test=0.753) total time=   1.1s\n",
      "[CV 2/5] END class_weight=None, max_iter=10000, penalty=None, solver=lbfgs;, score=(train=0.798, test=0.781) total time=   0.2s\n",
      "[CV 3/5] END class_weight=None, max_iter=10000, penalty=None, solver=lbfgs;, score=(train=0.799, test=0.775) total time=   0.1s\n",
      "[CV 3/5] END class_weight=None, max_iter=10000, penalty=None, solver=saga;, score=(train=0.801, test=0.787) total time=   1.1s\n",
      "[CV 1/5] END learning_rate=0.01, loss=log_loss, n_estimators=1000;, score=(train=0.893, test=0.793) total time=   2.4s\n",
      "[CV 1/5] END learning_rate=1, loss=log_loss, n_estimators=10;, score=(train=0.899, test=0.771) total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1, loss=log_loss, n_estimators=10;, score=(train=0.905, test=0.826) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1, loss=log_loss, n_estimators=1000;, score=(train=0.985, test=0.777) total time=   2.3s\n",
      "[CV 2/5] END learning_rate=1, loss=log_loss, n_estimators=1000;, score=(train=0.987, test=0.781) total time=   2.3s\n",
      "[CV 4/5] END learning_rate=100, loss=exponential, n_estimators=1000;, score=(train=0.205, test=0.247) total time=   1.4s\n",
      "[CV 5/5] END learning_rate=0.001, loss=log_loss, n_estimators=1000;, score=(train=0.836, test=0.848) total time=   2.2s\n",
      "[CV 1/5] END learning_rate=0.1, loss=log_loss, n_estimators=1000;, score=(train=0.982, test=0.771) total time=   2.2s\n",
      "[CV 2/5] END learning_rate=0.1, loss=log_loss, n_estimators=1000;, score=(train=0.985, test=0.820) total time=   2.1s\n",
      "[CV 4/5] END learning_rate=10, loss=exponential, n_estimators=1000;, score=(train=0.245, test=0.275) total time=   1.6s\n",
      "[CV 1/5] END class_weight=balanced, criterion=gini, max_depth=100, splitter=random;, score=(train=0.985, test=0.765) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=gini, max_depth=1000, splitter=best;, score=(train=0.985, test=0.721) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=gini, max_depth=1000, splitter=random;, score=(train=0.985, test=0.831) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=gini, max_depth=None, splitter=best;, score=(train=0.982, test=0.792) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=entropy, max_depth=100, splitter=random;, score=(train=0.987, test=0.764) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=entropy, max_depth=100, splitter=random;, score=(train=0.985, test=0.787) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=entropy, max_depth=None, splitter=best;, score=(train=0.985, test=0.732) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=entropy, max_depth=None, splitter=best;, score=(train=0.987, test=0.775) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=log_loss, max_depth=100, splitter=random;, score=(train=0.987, test=0.775) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=log_loss, max_depth=100, splitter=random;, score=(train=0.985, test=0.792) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=log_loss, max_depth=None, splitter=random;, score=(train=0.985, test=0.758) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=log_loss, max_depth=None, splitter=random;, score=(train=0.982, test=0.758) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=entropy, max_depth=100, splitter=best;, score=(train=0.985, test=0.754) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=entropy, max_depth=100, splitter=best;, score=(train=0.987, test=0.781) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=entropy, max_depth=100, splitter=best;, score=(train=0.985, test=0.792) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=entropy, max_depth=100, splitter=best;, score=(train=0.985, test=0.775) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=log_loss, max_depth=1000, splitter=random;, score=(train=0.985, test=0.749) total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=1000, n_estimators=10;, score=(train=0.969, test=0.837) total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=1000, n_estimators=1000;, score=(train=0.987, test=0.815) total time=   3.6s\n",
      "[CV 1/5] END criterion=log_loss, max_depth=1000, n_estimators=10;, score=(train=0.965, test=0.827) total time=   0.1s\n",
      "[CV 2/5] END criterion=log_loss, max_depth=1000, n_estimators=10;, score=(train=0.971, test=0.809) total time=   0.0s\n",
      "[CV 5/5] END criterion=log_loss, max_depth=1000, n_estimators=10;, score=(train=0.969, test=0.831) total time=   0.0s\n",
      "[CV 4/5] END criterion=log_loss, max_depth=1000, n_estimators=100;, score=(train=0.983, test=0.781) total time=   0.4s\n",
      "[CV 3/5] END criterion=log_loss, max_depth=None, n_estimators=10;, score=(train=0.965, test=0.831) total time=   0.1s\n",
      "[CV 2/5] END criterion=log_loss, max_depth=None, n_estimators=100;, score=(train=0.986, test=0.798) total time=   0.4s\n",
      "[CV 3/5] END criterion=log_loss, max_depth=None, n_estimators=1000;, score=(train=0.985, test=0.843) total time=   3.6s\n",
      "[CV 4/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=lbfgs;, score=(train=0.801, test=0.775) total time=   0.1s\n",
      "[CV 2/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=newton-cholesky;, score=(train=0.794, test=0.764) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=newton-cholesky;, score=(train=0.794, test=0.787) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=sag;, score=(train=0.789, test=0.743) total time=   0.8s\n",
      "[CV 2/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=sag;, score=(train=0.784, test=0.809) total time=   0.8s\n",
      "[CV 1/5] END learning_rate=0.001, loss=log_loss, n_estimators=10;, score=(train=0.617, test=0.615) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.001, loss=log_loss, n_estimators=100;, score=(train=0.617, test=0.615) total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.001, loss=exponential, n_estimators=1000;, score=(train=0.832, test=0.815) total time=   2.4s\n",
      "[CV 2/5] END learning_rate=0.1, loss=exponential, n_estimators=10;, score=(train=0.832, test=0.815) total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, loss=exponential, n_estimators=10;, score=(train=0.836, test=0.820) total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, loss=exponential, n_estimators=1000;, score=(train=0.985, test=0.809) total time=   2.3s\n",
      "[CV 3/5] END learning_rate=0.1, loss=exponential, n_estimators=1000;, score=(train=0.983, test=0.826) total time=   2.3s\n",
      "[CV 1/5] END class_weight=balanced, criterion=gini, max_depth=1000, splitter=random;, score=(train=0.985, test=0.749) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=gini, max_depth=100, splitter=best;, score=(train=0.985, test=0.815) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=gini, max_depth=1000, splitter=best;, score=(train=0.987, test=0.798) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=gini, max_depth=None, splitter=best;, score=(train=0.987, test=0.792) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=entropy, max_depth=100, splitter=best;, score=(train=0.985, test=0.726) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=entropy, max_depth=100, splitter=random;, score=(train=0.985, test=0.798) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=entropy, max_depth=100, splitter=random;, score=(train=0.982, test=0.815) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=log_loss, max_depth=100, splitter=best;, score=(train=0.985, test=0.737) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=log_loss, max_depth=100, splitter=best;, score=(train=0.987, test=0.775) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=log_loss, max_depth=None, splitter=best;, score=(train=0.985, test=0.726) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=log_loss, max_depth=None, splitter=best;, score=(train=0.987, test=0.775) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=gini, max_depth=None, splitter=random;, score=(train=0.987, test=0.770) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=gini, max_depth=None, splitter=random;, score=(train=0.985, test=0.770) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=gini, max_depth=None, splitter=random;, score=(train=0.985, test=0.758) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=gini, max_depth=None, splitter=random;, score=(train=0.982, test=0.764) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=log_loss, max_depth=1000, splitter=best;, score=(train=0.982, test=0.781) total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=1000, n_estimators=100;, score=(train=0.982, test=0.854) total time=   0.4s\n",
      "[CV 1/5] END criterion=entropy, max_depth=1000, n_estimators=1000;, score=(train=0.985, test=0.782) total time=   3.8s\n",
      "[CV 2/5] END criterion=log_loss, max_depth=1000, n_estimators=1000;, score=(train=0.987, test=0.815) total time=   3.6s\n",
      "[CV 2/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=lbfgs;, score=(train=0.795, test=0.764) total time=   0.1s\n",
      "[CV 2/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=lbfgs;, score=(train=0.792, test=0.764) total time=   0.2s\n",
      "[CV 2/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=newton-cg;, score=(train=0.794, test=0.764) total time=   0.1s\n",
      "[CV 2/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=saga;, score=(train=0.787, test=0.815) total time=   1.1s\n",
      "[CV 3/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=saga;, score=(train=0.784, test=0.792) total time=   1.0s\n",
      "[CV 4/5] END learning_rate=0.01, loss=log_loss, n_estimators=100;, score=(train=0.844, test=0.787) total time=   0.3s\n",
      "[CV 5/5] END learning_rate=0.01, loss=exponential, n_estimators=1000;, score=(train=0.896, test=0.854) total time=   2.4s\n",
      "[CV 1/5] END learning_rate=1, loss=exponential, n_estimators=100;, score=(train=0.983, test=0.788) total time=   0.3s\n",
      "[CV 2/5] END learning_rate=1, loss=exponential, n_estimators=100;, score=(train=0.985, test=0.820) total time=   0.3s\n",
      "[CV 3/5] END learning_rate=10, loss=log_loss, n_estimators=1000;, score=(train=0.613, test=0.601) total time=   2.2s\n",
      "[CV 4/5] END learning_rate=10, loss=log_loss, n_estimators=1000;, score=(train=0.236, test=0.236) total time=   2.3s\n",
      "[CV 3/5] END learning_rate=0.001, loss=log_loss, n_estimators=10;, score=(train=0.616, test=0.618) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.001, loss=log_loss, n_estimators=100;, score=(train=0.617, test=0.615) total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.001, loss=exponential, n_estimators=10;, score=(train=0.616, test=0.618) total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.001, loss=exponential, n_estimators=100;, score=(train=0.616, test=0.618) total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.01, loss=log_loss, n_estimators=100;, score=(train=0.844, test=0.787) total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.01, loss=log_loss, n_estimators=100;, score=(train=0.836, test=0.848) total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.01, loss=exponential, n_estimators=100;, score=(train=0.836, test=0.820) total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.01, loss=exponential, n_estimators=100;, score=(train=0.842, test=0.781) total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.01, loss=exponential, n_estimators=1000;, score=(train=0.896, test=0.787) total time=   2.3s\n",
      "[CV 5/5] END learning_rate=0.01, loss=exponential, n_estimators=1000;, score=(train=0.896, test=0.854) total time=   2.2s\n",
      "[CV 4/5] END learning_rate=10, loss=exponential, n_estimators=10;, score=(train=0.205, test=0.247) total time=   0.0s\n",
      "[CV 2/5] END learning_rate=10, loss=exponential, n_estimators=100;, score=(train=0.463, test=0.416) total time=   0.2s\n",
      "[CV 5/5] END learning_rate=10, loss=exponential, n_estimators=100;, score=(train=0.735, test=0.775) total time=   0.2s\n",
      "[CV 3/5] END learning_rate=10, loss=exponential, n_estimators=1000;, score=(train=0.359, test=0.320) total time=   1.7s\n",
      "[CV 4/5] END class_weight=None, criterion=gini, max_depth=1000, splitter=random;, score=(train=0.985, test=0.792) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=gini, max_depth=1000, splitter=random;, score=(train=0.982, test=0.792) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=gini, max_depth=None, splitter=best;, score=(train=0.985, test=0.743) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=gini, max_depth=None, splitter=best;, score=(train=0.987, test=0.775) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=log_loss, max_depth=1000, splitter=best;, score=(train=0.987, test=0.787) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=log_loss, max_depth=1000, splitter=random;, score=(train=0.985, test=0.781) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=log_loss, max_depth=None, splitter=random;, score=(train=0.985, test=0.747) total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=None, n_estimators=10;, score=(train=0.968, test=0.799) total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=None, n_estimators=10;, score=(train=0.976, test=0.787) total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=None, n_estimators=100;, score=(train=0.985, test=0.781) total time=   0.4s\n",
      "[CV 3/5] END criterion=entropy, max_depth=1000, n_estimators=1000;, score=(train=0.985, test=0.837) total time=   3.8s\n",
      "[CV 4/5] END criterion=entropy, max_depth=1000, n_estimators=1000;, score=(train=0.985, test=0.775) total time=   3.6s\n",
      "[CV 4/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=lbfgs;, score=(train=0.798, test=0.775) total time=   0.1s\n",
      "[CV 1/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=newton-cholesky;, score=(train=0.805, test=0.777) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=newton-cholesky;, score=(train=0.795, test=0.787) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=sag;, score=(train=0.787, test=0.787) total time=   0.7s\n",
      "[CV 3/5] END class_weight=None, max_iter=10000, penalty=l2, solver=newton-cholesky;, score=(train=0.801, test=0.781) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, max_iter=10000, penalty=l2, solver=newton-cholesky;, score=(train=0.806, test=0.770) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, max_iter=10000, penalty=l2, solver=newton-cholesky;, score=(train=0.797, test=0.820) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, max_iter=10000, penalty=l2, solver=sag;, score=(train=0.792, test=0.765) total time=   0.8s\n",
      "[CV 4/5] END class_weight=None, max_iter=10000, penalty=None, solver=saga;, score=(train=0.804, test=0.775) total time=   1.0s\n",
      "[CV 3/5] END learning_rate=0.001, loss=log_loss, n_estimators=10;, score=(train=0.616, test=0.618) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.001, loss=log_loss, n_estimators=100;, score=(train=0.616, test=0.618) total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.001, loss=exponential, n_estimators=1000;, score=(train=0.835, test=0.848) total time=   2.4s\n",
      "[CV 4/5] END learning_rate=0.1, loss=exponential, n_estimators=10;, score=(train=0.842, test=0.781) total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, loss=exponential, n_estimators=10;, score=(train=0.819, test=0.831) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, loss=exponential, n_estimators=1000;, score=(train=0.979, test=0.770) total time=   2.3s\n",
      "[CV 5/5] END learning_rate=0.1, loss=exponential, n_estimators=1000;, score=(train=0.978, test=0.820) total time=   2.3s\n",
      "[CV 2/5] END class_weight=balanced, criterion=gini, max_depth=1000, splitter=best;, score=(train=0.987, test=0.775) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=gini, max_depth=1000, splitter=random;, score=(train=0.982, test=0.809) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=gini, max_depth=None, splitter=random;, score=(train=0.985, test=0.742) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=entropy, max_depth=None, splitter=best;, score=(train=0.985, test=0.721) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=entropy, max_depth=None, splitter=best;, score=(train=0.987, test=0.764) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=log_loss, max_depth=100, splitter=random;, score=(train=0.987, test=0.781) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=log_loss, max_depth=100, splitter=random;, score=(train=0.985, test=0.775) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=gini, max_depth=100, splitter=best;, score=(train=0.982, test=0.809) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=gini, max_depth=100, splitter=random;, score=(train=0.985, test=0.709) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=entropy, max_depth=1000, splitter=random;, score=(train=0.987, test=0.770) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=entropy, max_depth=1000, splitter=random;, score=(train=0.985, test=0.820) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=entropy, max_depth=1000, splitter=random;, score=(train=0.985, test=0.781) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=entropy, max_depth=1000, splitter=random;, score=(train=0.982, test=0.787) total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=1000, n_estimators=100;, score=(train=0.985, test=0.854) total time=   0.4s\n",
      "[CV 4/5] END criterion=entropy, max_depth=1000, n_estimators=10;, score=(train=0.969, test=0.770) total time=   0.1s\n",
      "[CV 4/5] END criterion=entropy, max_depth=1000, n_estimators=100;, score=(train=0.985, test=0.770) total time=   0.4s\n",
      "[CV 5/5] END criterion=entropy, max_depth=1000, n_estimators=1000;, score=(train=0.982, test=0.860) total time=   3.8s\n",
      "[CV 5/5] END criterion=log_loss, max_depth=None, n_estimators=100;, score=(train=0.982, test=0.837) total time=   0.4s\n",
      "[CV 1/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=lbfgs;, score=(train=0.805, test=0.771) total time=   0.1s\n",
      "[CV 5/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=saga;, score=(train=0.780, test=0.803) total time=   1.1s\n",
      "[CV 4/5] END class_weight=None, max_iter=10000, penalty=None, solver=lbfgs;, score=(train=0.806, test=0.770) total time=   0.2s\n",
      "[CV 5/5] END class_weight=None, max_iter=10000, penalty=None, solver=lbfgs;, score=(train=0.802, test=0.826) total time=   0.1s\n",
      "[CV 3/5] END class_weight=None, max_iter=10000, penalty=None, solver=sag;, score=(train=0.804, test=0.781) total time=   0.8s\n",
      "[CV 4/5] END class_weight=None, max_iter=10000, penalty=None, solver=sag;, score=(train=0.798, test=0.764) total time=   0.5s\n",
      "[CV 5/5] END learning_rate=0.01, loss=log_loss, n_estimators=10;, score=(train=0.617, test=0.612) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.01, loss=exponential, n_estimators=10;, score=(train=0.616, test=0.618) total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.01, loss=exponential, n_estimators=100;, score=(train=0.835, test=0.848) total time=   0.3s\n",
      "[CV 1/5] END learning_rate=0.1, loss=log_loss, n_estimators=1000;, score=(train=0.982, test=0.771) total time=   2.4s\n",
      "[CV 2/5] END learning_rate=0.1, loss=log_loss, n_estimators=1000;, score=(train=0.985, test=0.820) total time=   2.3s\n",
      "[CV 5/5] END learning_rate=100, loss=log_loss, n_estimators=10;, score=(train=0.655, test=0.680) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=100, loss=log_loss, n_estimators=100;, score=(train=0.301, test=0.341) total time=   0.2s\n",
      "[CV 2/5] END learning_rate=100, loss=log_loss, n_estimators=1000;, score=(train=0.309, test=0.326) total time=   2.3s\n",
      "[CV 3/5] END learning_rate=0.001, loss=log_loss, n_estimators=1000;, score=(train=0.839, test=0.826) total time=   2.2s\n",
      "[CV 2/5] END learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.914, test=0.809) total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.895, test=0.843) total time=   0.2s\n",
      "[CV 5/5] END learning_rate=1, loss=log_loss, n_estimators=10;, score=(train=0.899, test=0.831) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1, loss=log_loss, n_estimators=100;, score=(train=0.985, test=0.788) total time=   0.2s\n",
      "[CV 5/5] END learning_rate=1, loss=log_loss, n_estimators=1000;, score=(train=0.982, test=0.798) total time=   2.2s\n",
      "[CV 1/5] END learning_rate=1, loss=exponential, n_estimators=10;, score=(train=0.878, test=0.788) total time=   0.0s\n",
      "[CV 5/5] END learning_rate=10, loss=log_loss, n_estimators=1000;, score=(train=0.631, test=0.646) total time=   2.1s\n",
      "[CV 3/5] END class_weight=balanced, criterion=gini, max_depth=100, splitter=best;, score=(train=0.985, test=0.826) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=gini, max_depth=100, splitter=random;, score=(train=0.985, test=0.787) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=gini, max_depth=1000, splitter=random;, score=(train=0.985, test=0.726) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=gini, max_depth=None, splitter=best;, score=(train=0.985, test=0.815) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=entropy, max_depth=100, splitter=best;, score=(train=0.982, test=0.781) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=entropy, max_depth=100, splitter=random;, score=(train=0.985, test=0.726) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=entropy, max_depth=1000, splitter=random;, score=(train=0.985, test=0.742) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=entropy, max_depth=1000, splitter=random;, score=(train=0.982, test=0.798) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=log_loss, max_depth=100, splitter=best;, score=(train=0.985, test=0.715) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=log_loss, max_depth=100, splitter=best;, score=(train=0.987, test=0.770) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=log_loss, max_depth=1000, splitter=random;, score=(train=0.987, test=0.781) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=log_loss, max_depth=1000, splitter=random;, score=(train=0.985, test=0.775) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=gini, max_depth=100, splitter=random;, score=(train=0.987, test=0.770) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=gini, max_depth=100, splitter=random;, score=(train=0.985, test=0.831) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=gini, max_depth=100, splitter=random;, score=(train=0.985, test=0.770) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=gini, max_depth=100, splitter=random;, score=(train=0.982, test=0.803) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=entropy, max_depth=None, splitter=best;, score=(train=0.982, test=0.787) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=entropy, max_depth=None, splitter=random;, score=(train=0.985, test=0.760) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=entropy, max_depth=None, splitter=random;, score=(train=0.987, test=0.764) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=entropy, max_depth=None, splitter=random;, score=(train=0.985, test=0.764) total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=1000, n_estimators=100;, score=(train=0.987, test=0.809) total time=   0.3s\n",
      "[CV 2/5] END criterion=gini, max_depth=None, n_estimators=1000;, score=(train=0.987, test=0.815) total time=   3.7s\n",
      "[CV 2/5] END criterion=log_loss, max_depth=1000, n_estimators=1000;, score=(train=0.987, test=0.820) total time=   3.7s\n",
      "[CV 3/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=lbfgs;, score=(train=0.794, test=0.787) total time=   0.2s\n",
      "[CV 5/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=newton-cg;, score=(train=0.787, test=0.787) total time=   0.1s\n",
      "[CV 1/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=newton-cholesky;, score=(train=0.805, test=0.765) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=saga;, score=(train=0.792, test=0.753) total time=   1.1s\n",
      "[CV 5/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=saga;, score=(train=0.780, test=0.803) total time=   1.0s\n",
      "[CV 4/5] END learning_rate=0.001, loss=log_loss, n_estimators=1000;, score=(train=0.844, test=0.787) total time=   2.3s\n",
      "[CV 5/5] END learning_rate=0.1, loss=log_loss, n_estimators=10;, score=(train=0.835, test=0.848) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.903, test=0.810) total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=0.892, test=0.854) total time=   0.2s\n",
      "[CV 1/5] END learning_rate=0.1, loss=exponential, n_estimators=1000;, score=(train=0.982, test=0.777) total time=   2.3s\n",
      "[CV 1/5] END learning_rate=10, loss=log_loss, n_estimators=1000;, score=(train=0.218, test=0.196) total time=   2.3s\n",
      "[CV 4/5] END class_weight=balanced, criterion=gini, max_depth=100, splitter=best;, score=(train=0.985, test=0.747) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=entropy, max_depth=1000, splitter=random;, score=(train=0.985, test=0.715) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=entropy, max_depth=1000, splitter=random;, score=(train=0.982, test=0.725) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=entropy, max_depth=None, splitter=best;, score=(train=0.985, test=0.736) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=entropy, max_depth=None, splitter=random;, score=(train=0.985, test=0.820) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=log_loss, max_depth=100, splitter=best;, score=(train=0.987, test=0.781) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=log_loss, max_depth=100, splitter=random;, score=(train=0.985, test=0.737) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=log_loss, max_depth=100, splitter=random;, score=(train=0.982, test=0.770) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=log_loss, max_depth=1000, splitter=best;, score=(train=0.982, test=0.770) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=log_loss, max_depth=1000, splitter=random;, score=(train=0.982, test=0.758) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=log_loss, max_depth=None, splitter=random;, score=(train=0.985, test=0.765) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=gini, max_depth=100, splitter=best;, score=(train=0.985, test=0.754) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=gini, max_depth=100, splitter=random;, score=(train=0.985, test=0.809) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=gini, max_depth=1000, splitter=best;, score=(train=0.987, test=0.775) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=gini, max_depth=1000, splitter=random;, score=(train=0.985, test=0.725) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=gini, max_depth=None, splitter=random;, score=(train=0.985, test=0.725) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=gini, max_depth=None, splitter=random;, score=(train=0.982, test=0.758) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=entropy, max_depth=1000, splitter=best;, score=(train=0.985, test=0.764) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=entropy, max_depth=1000, splitter=best;, score=(train=0.985, test=0.770) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=log_loss, max_depth=100, splitter=best;, score=(train=0.985, test=0.787) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=log_loss, max_depth=100, splitter=best;, score=(train=0.985, test=0.758) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=log_loss, max_depth=None, splitter=best;, score=(train=0.982, test=0.787) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=log_loss, max_depth=None, splitter=random;, score=(train=0.985, test=0.749) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=gini, max_depth=1000, splitter=best;, score=(train=0.982, test=0.792) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=gini, max_depth=None, splitter=random;, score=(train=0.985, test=0.771) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=entropy, max_depth=1000, splitter=best;, score=(train=0.985, test=0.709) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=entropy, max_depth=1000, splitter=best;, score=(train=0.987, test=0.758) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=log_loss, max_depth=100, splitter=best;, score=(train=0.982, test=0.781) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=log_loss, max_depth=100, splitter=random;, score=(train=0.985, test=0.777) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=log_loss, max_depth=None, splitter=random;, score=(train=0.985, test=0.736) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=log_loss, max_depth=None, splitter=random;, score=(train=0.982, test=0.809) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=log_loss, max_depth=100, splitter=best;, score=(train=0.985, test=0.775) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=log_loss, max_depth=100, splitter=best;, score=(train=0.985, test=0.758) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=log_loss, max_depth=100, splitter=best;, score=(train=0.982, test=0.787) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=log_loss, max_depth=100, splitter=random;, score=(train=0.985, test=0.771) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=log_loss, max_depth=None, splitter=random;, score=(train=0.985, test=0.775) total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=1000, n_estimators=10;, score=(train=0.972, test=0.848) total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=None, n_estimators=100;, score=(train=0.982, test=0.860) total time=   0.4s\n",
      "[CV 5/5] END criterion=entropy, max_depth=1000, n_estimators=10;, score=(train=0.964, test=0.843) total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=1000, n_estimators=100;, score=(train=0.980, test=0.837) total time=   0.4s\n",
      "[CV 1/5] END criterion=entropy, max_depth=None, n_estimators=10;, score=(train=0.973, test=0.765) total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=None, n_estimators=10;, score=(train=0.972, test=0.820) total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=None, n_estimators=10;, score=(train=0.966, test=0.876) total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=None, n_estimators=10;, score=(train=0.975, test=0.781) total time=   0.0s\n",
      "[CV 5/5] END criterion=entropy, max_depth=None, n_estimators=10;, score=(train=0.964, test=0.837) total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=None, n_estimators=100;, score=(train=0.985, test=0.788) total time=   0.4s\n",
      "[CV 2/5] END criterion=entropy, max_depth=None, n_estimators=100;, score=(train=0.987, test=0.803) total time=   0.4s\n",
      "[CV 3/5] END criterion=entropy, max_depth=None, n_estimators=100;, score=(train=0.985, test=0.831) total time=   0.4s\n",
      "[CV 4/5] END criterion=entropy, max_depth=None, n_estimators=100;, score=(train=0.985, test=0.764) total time=   0.4s\n",
      "[CV 5/5] END criterion=entropy, max_depth=None, n_estimators=100;, score=(train=0.982, test=0.865) total time=   0.4s\n",
      "[CV 1/5] END criterion=entropy, max_depth=None, n_estimators=1000;, score=(train=0.985, test=0.782) total time=   3.7s\n",
      "[CV 5/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=lbfgs;, score=(train=0.787, test=0.787) total time=   0.1s\n",
      "[CV 1/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=newton-cg;, score=(train=0.805, test=0.765) total time=   0.1s\n",
      "[CV 3/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=newton-cg;, score=(train=0.794, test=0.787) total time=   0.1s\n",
      "[CV 4/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=newton-cg;, score=(train=0.802, test=0.775) total time=   0.1s\n",
      "[CV 1/5] END class_weight=None, max_iter=10000, penalty=l2, solver=lbfgs;, score=(train=0.806, test=0.782) total time=   0.1s\n",
      "[CV 2/5] END class_weight=None, max_iter=10000, penalty=l2, solver=lbfgs;, score=(train=0.799, test=0.781) total time=   0.1s\n",
      "[CV 3/5] END class_weight=None, max_iter=10000, penalty=l2, solver=lbfgs;, score=(train=0.801, test=0.781) total time=   0.1s\n",
      "[CV 4/5] END class_weight=None, max_iter=10000, penalty=l2, solver=lbfgs;, score=(train=0.806, test=0.770) total time=   0.1s\n",
      "[CV 5/5] END class_weight=None, max_iter=10000, penalty=l2, solver=saga;, score=(train=0.795, test=0.815) total time=   1.1s\n",
      "[CV 1/5] END class_weight=None, max_iter=10000, penalty=None, solver=lbfgs;, score=(train=0.806, test=0.782) total time=   0.1s\n",
      "[CV 3/5] END learning_rate=0.01, loss=log_loss, n_estimators=100;, score=(train=0.839, test=0.826) total time=   0.3s\n",
      "[CV 2/5] END learning_rate=0.01, loss=exponential, n_estimators=1000;, score=(train=0.902, test=0.809) total time=   2.4s\n",
      "[CV 3/5] END learning_rate=1, loss=exponential, n_estimators=100;, score=(train=0.983, test=0.837) total time=   0.3s\n",
      "[CV 4/5] END learning_rate=1, loss=exponential, n_estimators=100;, score=(train=0.983, test=0.770) total time=   0.3s\n",
      "[CV 1/5] END learning_rate=10, loss=log_loss, n_estimators=1000;, score=(train=0.218, test=0.196) total time=   2.2s\n",
      "[CV 2/5] END learning_rate=10, loss=log_loss, n_estimators=1000;, score=(train=0.261, test=0.292) total time=   2.3s\n",
      "[CV 1/5] END learning_rate=0.01, loss=log_loss, n_estimators=10;, score=(train=0.617, test=0.615) total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.01, loss=log_loss, n_estimators=10;, score=(train=0.616, test=0.618) total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.01, loss=log_loss, n_estimators=10;, score=(train=0.616, test=0.618) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.01, loss=log_loss, n_estimators=10;, score=(train=0.616, test=0.618) total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.01, loss=log_loss, n_estimators=10;, score=(train=0.617, test=0.612) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.01, loss=log_loss, n_estimators=100;, score=(train=0.833, test=0.810) total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.01, loss=exponential, n_estimators=10;, score=(train=0.616, test=0.618) total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.01, loss=exponential, n_estimators=10;, score=(train=0.616, test=0.618) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.01, loss=exponential, n_estimators=10;, score=(train=0.616, test=0.618) total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.01, loss=exponential, n_estimators=10;, score=(train=0.617, test=0.612) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.01, loss=exponential, n_estimators=100;, score=(train=0.831, test=0.810) total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.01, loss=exponential, n_estimators=100;, score=(train=0.832, test=0.815) total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.01, loss=exponential, n_estimators=1000;, score=(train=0.902, test=0.809) total time=   2.3s\n",
      "[CV 3/5] END learning_rate=0.01, loss=exponential, n_estimators=1000;, score=(train=0.879, test=0.843) total time=   2.2s\n",
      "[CV 1/5] END learning_rate=10, loss=exponential, n_estimators=10;, score=(train=0.247, test=0.212) total time=   0.0s\n",
      "[CV 2/5] END learning_rate=10, loss=exponential, n_estimators=10;, score=(train=0.463, test=0.416) total time=   0.0s\n",
      "[CV 3/5] END learning_rate=10, loss=exponential, n_estimators=10;, score=(train=0.387, test=0.348) total time=   0.0s\n",
      "[CV 5/5] END learning_rate=10, loss=exponential, n_estimators=10;, score=(train=0.735, test=0.770) total time=   0.0s\n",
      "[CV 3/5] END learning_rate=10, loss=exponential, n_estimators=100;, score=(train=0.351, test=0.315) total time=   0.2s\n",
      "[CV 1/5] END learning_rate=10, loss=exponential, n_estimators=1000;, score=(train=0.496, test=0.458) total time=   1.8s\n",
      "[CV 1/5] END class_weight=None, criterion=log_loss, max_depth=None, splitter=best;, score=(train=0.985, test=0.737) total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=1000, n_estimators=1000;, score=(train=0.985, test=0.770) total time=   3.6s\n",
      "[CV 1/5] END criterion=log_loss, max_depth=1000, n_estimators=100;, score=(train=0.985, test=0.782) total time=   0.4s\n",
      "[CV 1/5] END criterion=log_loss, max_depth=None, n_estimators=10;, score=(train=0.968, test=0.782) total time=   0.0s\n",
      "[CV 5/5] END criterion=log_loss, max_depth=None, n_estimators=10;, score=(train=0.971, test=0.843) total time=   0.0s\n",
      "[CV 4/5] END criterion=log_loss, max_depth=None, n_estimators=100;, score=(train=0.985, test=0.781) total time=   0.4s\n",
      "[CV 4/5] END criterion=log_loss, max_depth=None, n_estimators=1000;, score=(train=0.985, test=0.764) total time=   3.5s\n",
      "[CV 1/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=lbfgs;, score=(train=0.805, test=0.760) total time=   0.1s\n",
      "[CV 2/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=newton-cg;, score=(train=0.794, test=0.764) total time=   0.1s\n",
      "[CV 2/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=saga;, score=(train=0.787, test=0.815) total time=   1.1s\n",
      "[CV 3/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=saga;, score=(train=0.784, test=0.792) total time=   1.0s\n",
      "[CV 5/5] END learning_rate=0.001, loss=log_loss, n_estimators=100;, score=(train=0.617, test=0.612) total time=   0.2s\n",
      "[CV 1/5] END learning_rate=0.01, loss=log_loss, n_estimators=10;, score=(train=0.617, test=0.615) total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.01, loss=log_loss, n_estimators=10;, score=(train=0.616, test=0.618) total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.01, loss=log_loss, n_estimators=10;, score=(train=0.616, test=0.618) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.01, loss=log_loss, n_estimators=10;, score=(train=0.616, test=0.618) total time=   0.1s\n",
      "[CV 3/5] END learning_rate=0.01, loss=log_loss, n_estimators=1000;, score=(train=0.898, test=0.848) total time=   2.2s\n",
      "[CV 4/5] END learning_rate=0.01, loss=log_loss, n_estimators=1000;, score=(train=0.907, test=0.803) total time=   2.2s\n",
      "[CV 3/5] END learning_rate=10, loss=log_loss, n_estimators=10;, score=(train=0.613, test=0.601) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=10, loss=log_loss, n_estimators=10;, score=(train=0.236, test=0.236) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=10, loss=log_loss, n_estimators=100;, score=(train=0.236, test=0.236) total time=   0.2s\n",
      "[CV 2/5] END learning_rate=10, loss=log_loss, n_estimators=1000;, score=(train=0.261, test=0.292) total time=   2.3s\n",
      "[CV 3/5] END class_weight=balanced, criterion=gini, max_depth=100, splitter=random;, score=(train=0.985, test=0.831) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=entropy, max_depth=100, splitter=random;, score=(train=0.987, test=0.775) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=entropy, max_depth=100, splitter=random;, score=(train=0.985, test=0.815) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=entropy, max_depth=1000, splitter=random;, score=(train=0.985, test=0.781) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=entropy, max_depth=1000, splitter=random;, score=(train=0.982, test=0.803) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=log_loss, max_depth=100, splitter=random;, score=(train=0.987, test=0.764) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=log_loss, max_depth=100, splitter=random;, score=(train=0.985, test=0.860) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=log_loss, max_depth=None, splitter=best;, score=(train=0.985, test=0.775) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=log_loss, max_depth=None, splitter=best;, score=(train=0.985, test=0.770) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=gini, max_depth=1000, splitter=best;, score=(train=0.985, test=0.826) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=gini, max_depth=None, splitter=best;, score=(train=0.985, test=0.826) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=entropy, max_depth=100, splitter=best;, score=(train=0.987, test=0.753) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=entropy, max_depth=1000, splitter=random;, score=(train=0.987, test=0.758) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=entropy, max_depth=1000, splitter=random;, score=(train=0.985, test=0.787) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=log_loss, max_depth=100, splitter=random;, score=(train=0.985, test=0.753) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=log_loss, max_depth=100, splitter=random;, score=(train=0.982, test=0.758) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=gini, max_depth=100, splitter=random;, score=(train=0.987, test=0.770) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=gini, max_depth=100, splitter=random;, score=(train=0.985, test=0.809) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=gini, max_depth=100, splitter=random;, score=(train=0.985, test=0.770) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=gini, max_depth=100, splitter=random;, score=(train=0.982, test=0.753) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=log_loss, max_depth=1000, splitter=best;, score=(train=0.985, test=0.749) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=log_loss, max_depth=1000, splitter=random;, score=(train=0.987, test=0.742) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=log_loss, max_depth=None, splitter=best;, score=(train=0.982, test=0.798) total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=1000, n_estimators=100;, score=(train=0.985, test=0.788) total time=   0.4s\n",
      "[CV 1/5] END criterion=entropy, max_depth=1000, n_estimators=100;, score=(train=0.985, test=0.788) total time=   0.4s\n",
      "[CV 2/5] END criterion=entropy, max_depth=1000, n_estimators=1000;, score=(train=0.987, test=0.798) total time=   3.8s\n",
      "[CV 2/5] END criterion=log_loss, max_depth=None, n_estimators=100;, score=(train=0.987, test=0.815) total time=   0.4s\n",
      "[CV 3/5] END criterion=log_loss, max_depth=None, n_estimators=1000;, score=(train=0.985, test=0.843) total time=   3.2s\n",
      "[CV 2/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=newton-cg;, score=(train=0.795, test=0.764) total time=   0.1s\n",
      "[CV 3/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=saga;, score=(train=0.787, test=0.792) total time=   1.1s\n",
      "[CV 3/5] END class_weight=None, max_iter=10000, penalty=None, solver=newton-cg;, score=(train=0.799, test=0.775) total time=   0.1s\n",
      "[CV 4/5] END class_weight=None, max_iter=10000, penalty=None, solver=newton-cg;, score=(train=0.806, test=0.770) total time=   0.1s\n",
      "[CV 2/5] END class_weight=None, max_iter=10000, penalty=None, solver=saga;, score=(train=0.780, test=0.787) total time=   1.1s\n",
      "[CV 3/5] END learning_rate=0.01, loss=log_loss, n_estimators=1000;, score=(train=0.898, test=0.848) total time=   2.4s\n",
      "[CV 5/5] END learning_rate=1, loss=log_loss, n_estimators=10;, score=(train=0.899, test=0.831) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1, loss=log_loss, n_estimators=100;, score=(train=0.985, test=0.799) total time=   0.2s\n",
      "[CV 2/5] END learning_rate=1, loss=exponential, n_estimators=10;, score=(train=0.879, test=0.809) total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1, loss=exponential, n_estimators=10;, score=(train=0.877, test=0.831) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1, loss=exponential, n_estimators=1000;, score=(train=0.985, test=0.764) total time=   2.3s\n",
      "[CV 5/5] END learning_rate=1, loss=exponential, n_estimators=1000;, score=(train=0.982, test=0.803) total time=   2.4s\n",
      "[CV 3/5] END learning_rate=0.001, loss=exponential, n_estimators=10;, score=(train=0.616, test=0.618) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.001, loss=exponential, n_estimators=1000;, score=(train=0.831, test=0.810) total time=   2.3s\n",
      "[CV 5/5] END learning_rate=0.1, loss=log_loss, n_estimators=1000;, score=(train=0.978, test=0.820) total time=   2.2s\n",
      "[CV 1/5] END learning_rate=0.1, loss=exponential, n_estimators=10;, score=(train=0.831, test=0.810) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=10, loss=log_loss, n_estimators=10;, score=(train=0.218, test=0.196) total time=   0.0s\n",
      "[CV 2/5] END learning_rate=10, loss=log_loss, n_estimators=10;, score=(train=0.261, test=0.292) total time=   0.0s\n",
      "[CV 5/5] END learning_rate=10, loss=log_loss, n_estimators=10;, score=(train=0.631, test=0.646) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=10, loss=log_loss, n_estimators=100;, score=(train=0.218, test=0.196) total time=   0.2s\n",
      "[CV 1/5] END learning_rate=10, loss=log_loss, n_estimators=1000;, score=(train=0.218, test=0.196) total time=   2.2s\n",
      "[CV 2/5] END class_weight=None, criterion=log_loss, max_depth=100, splitter=random;, score=(train=0.987, test=0.803) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=log_loss, max_depth=100, splitter=random;, score=(train=0.985, test=0.843) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=log_loss, max_depth=100, splitter=random;, score=(train=0.985, test=0.770) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=log_loss, max_depth=100, splitter=random;, score=(train=0.982, test=0.815) total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=None, n_estimators=100;, score=(train=0.987, test=0.809) total time=   0.4s\n",
      "[CV 2/5] END criterion=entropy, max_depth=1000, n_estimators=100;, score=(train=0.987, test=0.815) total time=   0.4s\n",
      "[CV 3/5] END criterion=entropy, max_depth=1000, n_estimators=100;, score=(train=0.985, test=0.837) total time=   0.4s\n",
      "[CV 3/5] END criterion=entropy, max_depth=None, n_estimators=100;, score=(train=0.985, test=0.820) total time=   0.4s\n",
      "[CV 4/5] END criterion=entropy, max_depth=None, n_estimators=100;, score=(train=0.985, test=0.775) total time=   0.4s\n",
      "[CV 4/5] END criterion=entropy, max_depth=None, n_estimators=1000;, score=(train=0.985, test=0.775) total time=   3.9s\n",
      "[CV 5/5] END criterion=entropy, max_depth=None, n_estimators=1000;, score=(train=0.982, test=0.837) total time=   3.0s\n",
      "[CV 2/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=lbfgs;, score=(train=0.795, test=0.764) total time=   0.1s\n",
      "[CV 3/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=newton-cg;, score=(train=0.795, test=0.787) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=sag;, score=(train=0.789, test=0.743) total time=   0.7s\n",
      "[CV 4/5] END class_weight=None, max_iter=10000, penalty=l2, solver=newton-cg;, score=(train=0.806, test=0.770) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, max_iter=10000, penalty=l2, solver=newton-cg;, score=(train=0.797, test=0.820) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, max_iter=10000, penalty=l2, solver=saga;, score=(train=0.799, test=0.749) total time=   1.2s\n",
      "[CV 2/5] END class_weight=None, max_iter=10000, penalty=l2, solver=saga;, score=(train=0.777, test=0.787) total time=   0.8s\n",
      "[CV 2/5] END learning_rate=0.001, loss=log_loss, n_estimators=10;, score=(train=0.616, test=0.618) total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.001, loss=log_loss, n_estimators=100;, score=(train=0.616, test=0.618) total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.001, loss=exponential, n_estimators=1000;, score=(train=0.836, test=0.820) total time=   2.4s\n",
      "[CV 1/5] END learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=0.889, test=0.804) total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=0.905, test=0.820) total time=   0.2s\n",
      "[CV 1/5] END learning_rate=1, loss=log_loss, n_estimators=1000;, score=(train=0.985, test=0.777) total time=   2.2s\n",
      "[CV 2/5] END learning_rate=1, loss=log_loss, n_estimators=1000;, score=(train=0.987, test=0.775) total time=   2.2s\n",
      "[CV 5/5] END class_weight=balanced, criterion=gini, max_depth=100, splitter=best;, score=(train=0.982, test=0.792) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=log_loss, max_depth=100, splitter=random;, score=(train=0.987, test=0.758) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=log_loss, max_depth=1000, splitter=best;, score=(train=0.987, test=0.775) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=log_loss, max_depth=1000, splitter=random;, score=(train=0.987, test=0.758) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=log_loss, max_depth=None, splitter=best;, score=(train=0.985, test=0.820) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=log_loss, max_depth=None, splitter=random;, score=(train=0.985, test=0.775) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=gini, max_depth=100, splitter=random;, score=(train=0.985, test=0.732) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=gini, max_depth=1000, splitter=best;, score=(train=0.985, test=0.747) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=gini, max_depth=None, splitter=best;, score=(train=0.985, test=0.732) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=entropy, max_depth=100, splitter=best;, score=(train=0.982, test=0.787) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=entropy, max_depth=100, splitter=random;, score=(train=0.985, test=0.743) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=entropy, max_depth=None, splitter=best;, score=(train=0.985, test=0.775) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=entropy, max_depth=None, splitter=best;, score=(train=0.985, test=0.770) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=log_loss, max_depth=1000, splitter=best;, score=(train=0.985, test=0.760) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=log_loss, max_depth=1000, splitter=best;, score=(train=0.987, test=0.781) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=log_loss, max_depth=None, splitter=random;, score=(train=0.985, test=0.775) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=entropy, max_depth=1000, splitter=random;, score=(train=0.985, test=0.764) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=entropy, max_depth=1000, splitter=random;, score=(train=0.982, test=0.787) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=log_loss, max_depth=1000, splitter=best;, score=(train=0.982, test=0.775) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=log_loss, max_depth=1000, splitter=random;, score=(train=0.985, test=0.737) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=entropy, max_depth=None, splitter=best;, score=(train=0.982, test=0.781) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=entropy, max_depth=None, splitter=random;, score=(train=0.985, test=0.760) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=entropy, max_depth=None, splitter=random;, score=(train=0.987, test=0.770) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=entropy, max_depth=None, splitter=random;, score=(train=0.985, test=0.826) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=log_loss, max_depth=None, splitter=best;, score=(train=0.985, test=0.758) total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=1000, n_estimators=10;, score=(train=0.972, test=0.815) total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=None, n_estimators=10;, score=(train=0.968, test=0.831) total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=None, n_estimators=100;, score=(train=0.985, test=0.788) total time=   0.3s\n",
      "[CV 2/5] END criterion=gini, max_depth=None, n_estimators=1000;, score=(train=0.987, test=0.815) total time=   3.7s\n",
      "[CV 1/5] END criterion=log_loss, max_depth=1000, n_estimators=10;, score=(train=0.968, test=0.771) total time=   0.0s\n",
      "[CV 4/5] END criterion=log_loss, max_depth=1000, n_estimators=10;, score=(train=0.973, test=0.770) total time=   0.0s\n",
      "[CV 5/5] END criterion=log_loss, max_depth=1000, n_estimators=100;, score=(train=0.982, test=0.837) total time=   0.4s\n",
      "[CV 2/5] END criterion=log_loss, max_depth=None, n_estimators=10;, score=(train=0.965, test=0.809) total time=   0.0s\n",
      "[CV 4/5] END criterion=log_loss, max_depth=None, n_estimators=10;, score=(train=0.966, test=0.770) total time=   0.0s\n",
      "[CV 1/5] END criterion=log_loss, max_depth=None, n_estimators=100;, score=(train=0.985, test=0.793) total time=   0.4s\n",
      "[CV 2/5] END criterion=log_loss, max_depth=None, n_estimators=1000;, score=(train=0.987, test=0.815) total time=   3.2s\n",
      "[CV 1/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=newton-cg;, score=(train=0.805, test=0.777) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=sag;, score=(train=0.787, test=0.787) total time=   0.8s\n",
      "[CV 3/5] END class_weight=None, max_iter=10000, penalty=l2, solver=newton-cholesky;, score=(train=0.801, test=0.781) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, max_iter=10000, penalty=l2, solver=newton-cholesky;, score=(train=0.806, test=0.770) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, max_iter=10000, penalty=l2, solver=sag;, score=(train=0.791, test=0.798) total time=   0.8s\n",
      "[CV 3/5] END class_weight=None, max_iter=10000, penalty=l2, solver=sag;, score=(train=0.802, test=0.781) total time=   0.7s\n",
      "[CV 5/5] END learning_rate=0.01, loss=log_loss, n_estimators=100;, score=(train=0.836, test=0.848) total time=   0.3s\n",
      "[CV 1/5] END learning_rate=0.1, loss=log_loss, n_estimators=10;, score=(train=0.831, test=0.810) total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, loss=log_loss, n_estimators=10;, score=(train=0.832, test=0.815) total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, loss=log_loss, n_estimators=10;, score=(train=0.835, test=0.848) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.903, test=0.804) total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.1, loss=exponential, n_estimators=10;, score=(train=0.832, test=0.815) total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, loss=exponential, n_estimators=10;, score=(train=0.836, test=0.820) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, loss=exponential, n_estimators=10;, score=(train=0.842, test=0.781) total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, loss=exponential, n_estimators=10;, score=(train=0.819, test=0.831) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=0.889, test=0.804) total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=0.905, test=0.820) total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.1, loss=exponential, n_estimators=1000;, score=(train=0.985, test=0.809) total time=   2.5s\n",
      "[CV 3/5] END learning_rate=0.1, loss=exponential, n_estimators=1000;, score=(train=0.983, test=0.826) total time=   2.3s\n",
      "[CV 1/5] END learning_rate=100, loss=exponential, n_estimators=10;, score=(train=0.229, test=0.218) total time=   0.0s\n",
      "[CV 2/5] END learning_rate=100, loss=exponential, n_estimators=10;, score=(train=0.391, test=0.399) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=100, loss=exponential, n_estimators=10;, score=(train=0.205, test=0.247) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=100, loss=exponential, n_estimators=100;, score=(train=0.228, test=0.291) total time=   0.2s\n",
      "[CV 4/5] END learning_rate=100, loss=exponential, n_estimators=100;, score=(train=0.205, test=0.247) total time=   0.2s\n",
      "[CV 2/5] END learning_rate=100, loss=exponential, n_estimators=1000;, score=(train=0.390, test=0.399) total time=   1.8s\n",
      "[CV 4/5] END learning_rate=0.001, loss=exponential, n_estimators=1000;, score=(train=0.842, test=0.781) total time=   2.3s\n",
      "[CV 1/5] END learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=0.889, test=0.804) total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=0.905, test=0.820) total time=   0.3s\n",
      "[CV 1/5] END learning_rate=1, loss=log_loss, n_estimators=1000;, score=(train=0.985, test=0.777) total time=   2.2s\n",
      "[CV 2/5] END learning_rate=1, loss=log_loss, n_estimators=1000;, score=(train=0.987, test=0.775) total time=   2.1s\n",
      "[CV 3/5] END class_weight=None, criterion=log_loss, max_depth=1000, splitter=best;, score=(train=0.985, test=0.787) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=log_loss, max_depth=None, splitter=random;, score=(train=0.985, test=0.737) total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=1000, n_estimators=1000;, score=(train=0.982, test=0.843) total time=   3.7s\n",
      "[CV 5/5] END criterion=log_loss, max_depth=1000, n_estimators=100;, score=(train=0.982, test=0.848) total time=   0.4s\n",
      "[CV 4/5] END criterion=log_loss, max_depth=None, n_estimators=10;, score=(train=0.969, test=0.781) total time=   0.1s\n",
      "[CV 3/5] END criterion=log_loss, max_depth=None, n_estimators=100;, score=(train=0.985, test=0.837) total time=   0.4s\n",
      "[CV 5/5] END criterion=log_loss, max_depth=None, n_estimators=1000;, score=(train=0.982, test=0.843) total time=   3.6s\n",
      "[CV 2/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=newton-cg;, score=(train=0.795, test=0.764) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=newton-cholesky;, score=(train=0.788, test=0.787) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=sag;, score=(train=0.778, test=0.803) total time=   0.7s\n",
      "[CV 2/5] END class_weight=None, max_iter=10000, penalty=l2, solver=newton-cg;, score=(train=0.799, test=0.781) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, max_iter=10000, penalty=l2, solver=newton-cg;, score=(train=0.801, test=0.781) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, max_iter=10000, penalty=l2, solver=sag;, score=(train=0.799, test=0.764) total time=   0.8s\n",
      "[CV 5/5] END class_weight=None, max_iter=10000, penalty=l2, solver=sag;, score=(train=0.791, test=0.815) total time=   0.7s\n",
      "[CV 3/5] END learning_rate=0.001, loss=exponential, n_estimators=10;, score=(train=0.616, test=0.618) total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.001, loss=exponential, n_estimators=100;, score=(train=0.616, test=0.618) total time=   0.3s\n",
      "[CV 1/5] END learning_rate=0.01, loss=log_loss, n_estimators=1000;, score=(train=0.893, test=0.793) total time=   2.2s\n",
      "[CV 2/5] END learning_rate=0.01, loss=log_loss, n_estimators=1000;, score=(train=0.910, test=0.837) total time=   2.2s\n",
      "[CV 5/5] END learning_rate=10, loss=log_loss, n_estimators=10;, score=(train=0.631, test=0.646) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=10, loss=log_loss, n_estimators=100;, score=(train=0.218, test=0.196) total time=   0.2s\n",
      "[CV 5/5] END learning_rate=10, loss=log_loss, n_estimators=100;, score=(train=0.631, test=0.646) total time=   0.2s\n",
      "[CV 3/5] END learning_rate=10, loss=log_loss, n_estimators=1000;, score=(train=0.613, test=0.601) total time=   2.2s\n",
      "[CV 5/5] END class_weight=balanced, criterion=gini, max_depth=100, splitter=random;, score=(train=0.982, test=0.787) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=entropy, max_depth=None, splitter=random;, score=(train=0.987, test=0.758) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=entropy, max_depth=None, splitter=random;, score=(train=0.985, test=0.803) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=log_loss, max_depth=1000, splitter=random;, score=(train=0.987, test=0.753) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=log_loss, max_depth=1000, splitter=random;, score=(train=0.985, test=0.798) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=gini, max_depth=1000, splitter=random;, score=(train=0.985, test=0.764) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=gini, max_depth=1000, splitter=random;, score=(train=0.982, test=0.803) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=gini, max_depth=None, splitter=best;, score=(train=0.985, test=0.726) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=gini, max_depth=None, splitter=best;, score=(train=0.987, test=0.787) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=log_loss, max_depth=1000, splitter=best;, score=(train=0.985, test=0.764) total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=1000, n_estimators=1000;, score=(train=0.985, test=0.775) total time=   3.8s\n",
      "[CV 5/5] END criterion=entropy, max_depth=None, n_estimators=1000;, score=(train=0.982, test=0.854) total time=   3.7s\n",
      "[CV 5/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=newton-cg;, score=(train=0.788, test=0.787) total time=   0.1s\n",
      "[CV 1/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=lbfgs;, score=(train=0.805, test=0.760) total time=   0.2s\n",
      "[CV 4/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=newton-cholesky;, score=(train=0.802, test=0.775) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=newton-cholesky;, score=(train=0.787, test=0.787) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=sag;, score=(train=0.778, test=0.803) total time=   0.7s\n",
      "[CV 1/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=saga;, score=(train=0.792, test=0.743) total time=   1.1s\n",
      "[CV 5/5] END learning_rate=0.01, loss=log_loss, n_estimators=1000;, score=(train=0.903, test=0.848) total time=   2.4s\n",
      "[CV 2/5] END learning_rate=1, loss=log_loss, n_estimators=100;, score=(train=0.987, test=0.781) total time=   0.3s\n",
      "[CV 3/5] END learning_rate=1, loss=log_loss, n_estimators=100;, score=(train=0.985, test=0.831) total time=   0.3s\n",
      "[CV 2/5] END learning_rate=10, loss=log_loss, n_estimators=100;, score=(train=0.261, test=0.292) total time=   0.3s\n",
      "[CV 3/5] END learning_rate=10, loss=log_loss, n_estimators=100;, score=(train=0.613, test=0.601) total time=   0.2s\n",
      "[CV 2/5] END learning_rate=10, loss=exponential, n_estimators=10;, score=(train=0.463, test=0.416) total time=   0.0s\n",
      "[CV 3/5] END learning_rate=10, loss=exponential, n_estimators=10;, score=(train=0.387, test=0.348) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=10, loss=exponential, n_estimators=100;, score=(train=0.256, test=0.218) total time=   0.2s\n",
      "[CV 2/5] END learning_rate=10, loss=exponential, n_estimators=100;, score=(train=0.463, test=0.416) total time=   0.2s\n",
      "[CV 2/5] END learning_rate=10, loss=exponential, n_estimators=1000;, score=(train=0.506, test=0.461) total time=   2.3s\n",
      "[CV 3/5] END learning_rate=10, loss=exponential, n_estimators=1000;, score=(train=0.359, test=0.320) total time=   2.0s\n",
      "[CV 2/5] END learning_rate=0.001, loss=exponential, n_estimators=1000;, score=(train=0.832, test=0.815) total time=   2.3s\n",
      "[CV 4/5] END learning_rate=0.1, loss=exponential, n_estimators=10;, score=(train=0.842, test=0.781) total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, loss=exponential, n_estimators=10;, score=(train=0.819, test=0.831) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, loss=exponential, n_estimators=1000;, score=(train=0.979, test=0.775) total time=   2.3s\n",
      "[CV 5/5] END learning_rate=0.1, loss=exponential, n_estimators=1000;, score=(train=0.978, test=0.820) total time=   2.2s\n",
      "[CV 1/5] END class_weight=balanced, criterion=gini, max_depth=100, splitter=best;, score=(train=0.985, test=0.737) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=gini, max_depth=100, splitter=random;, score=(train=0.987, test=0.753) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=gini, max_depth=1000, splitter=best;, score=(train=0.987, test=0.781) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=gini, max_depth=1000, splitter=random;, score=(train=0.985, test=0.753) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=gini, max_depth=None, splitter=random;, score=(train=0.987, test=0.764) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=entropy, max_depth=100, splitter=best;, score=(train=0.987, test=0.770) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=entropy, max_depth=1000, splitter=random;, score=(train=0.987, test=0.798) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=entropy, max_depth=1000, splitter=random;, score=(train=0.985, test=0.831) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=entropy, max_depth=None, splitter=random;, score=(train=0.987, test=0.781) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=entropy, max_depth=None, splitter=random;, score=(train=0.985, test=0.787) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=log_loss, max_depth=100, splitter=random;, score=(train=0.985, test=0.747) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=log_loss, max_depth=100, splitter=random;, score=(train=0.982, test=0.770) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=log_loss, max_depth=None, splitter=random;, score=(train=0.987, test=0.742) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=log_loss, max_depth=None, splitter=random;, score=(train=0.985, test=0.803) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=entropy, max_depth=100, splitter=best;, score=(train=0.982, test=0.787) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=entropy, max_depth=100, splitter=random;, score=(train=0.985, test=0.765) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=entropy, max_depth=100, splitter=random;, score=(train=0.987, test=0.792) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=entropy, max_depth=100, splitter=random;, score=(train=0.985, test=0.792) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=log_loss, max_depth=1000, splitter=best;, score=(train=0.982, test=0.781) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=log_loss, max_depth=None, splitter=random;, score=(train=0.985, test=0.820) total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=1000, n_estimators=10;, score=(train=0.952, test=0.843) total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=1000, n_estimators=100;, score=(train=0.982, test=0.848) total time=   0.3s\n",
      "[CV 1/5] END criterion=entropy, max_depth=1000, n_estimators=10;, score=(train=0.963, test=0.754) total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=1000, n_estimators=10;, score=(train=0.976, test=0.820) total time=   0.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=1000, n_estimators=10;, score=(train=0.961, test=0.843) total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=1000, n_estimators=10;, score=(train=0.968, test=0.770) total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=1000, n_estimators=100;, score=(train=0.985, test=0.764) total time=   0.4s\n",
      "[CV 5/5] END criterion=entropy, max_depth=1000, n_estimators=100;, score=(train=0.982, test=0.837) total time=   0.4s\n",
      "[CV 5/5] END criterion=entropy, max_depth=None, n_estimators=100;, score=(train=0.982, test=0.848) total time=   0.4s\n",
      "[CV 1/5] END criterion=entropy, max_depth=None, n_estimators=1000;, score=(train=0.985, test=0.782) total time=   3.9s\n",
      "[CV 3/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=saga;, score=(train=0.787, test=0.792) total time=   1.1s\n",
      "[CV 3/5] END class_weight=None, max_iter=10000, penalty=None, solver=newton-cg;, score=(train=0.799, test=0.775) total time=   0.1s\n",
      "[CV 4/5] END class_weight=None, max_iter=10000, penalty=None, solver=newton-cg;, score=(train=0.806, test=0.770) total time=   0.1s\n",
      "[CV 2/5] END class_weight=None, max_iter=10000, penalty=None, solver=saga;, score=(train=0.780, test=0.787) total time=   1.1s\n",
      "[CV 5/5] END learning_rate=0.001, loss=exponential, n_estimators=10;, score=(train=0.617, test=0.612) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.001, loss=exponential, n_estimators=1000;, score=(train=0.831, test=0.810) total time=   2.4s\n",
      "[CV 5/5] END learning_rate=0.1, loss=log_loss, n_estimators=1000;, score=(train=0.978, test=0.820) total time=   2.2s\n",
      "[CV 1/5] END learning_rate=0.1, loss=exponential, n_estimators=10;, score=(train=0.831, test=0.810) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=10, loss=log_loss, n_estimators=10;, score=(train=0.218, test=0.196) total time=   0.0s\n",
      "[CV 2/5] END learning_rate=10, loss=log_loss, n_estimators=10;, score=(train=0.261, test=0.292) total time=   0.0s\n",
      "[CV 2/5] END learning_rate=10, loss=log_loss, n_estimators=100;, score=(train=0.261, test=0.292) total time=   0.2s\n",
      "[CV 3/5] END learning_rate=10, loss=log_loss, n_estimators=100;, score=(train=0.613, test=0.601) total time=   0.2s\n",
      "[CV 4/5] END learning_rate=10, loss=log_loss, n_estimators=1000;, score=(train=0.236, test=0.236) total time=   2.2s\n",
      "[CV 4/5] END class_weight=balanced, criterion=gini, max_depth=1000, splitter=best;, score=(train=0.985, test=0.736) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=entropy, max_depth=100, splitter=best;, score=(train=0.985, test=0.737) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=entropy, max_depth=100, splitter=best;, score=(train=0.987, test=0.787) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=entropy, max_depth=100, splitter=best;, score=(train=0.985, test=0.792) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=entropy, max_depth=100, splitter=best;, score=(train=0.985, test=0.753) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=log_loss, max_depth=None, splitter=best;, score=(train=0.987, test=0.775) total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=1000, n_estimators=100;, score=(train=0.985, test=0.770) total time=   0.3s\n",
      "[CV 3/5] END criterion=gini, max_depth=None, n_estimators=1000;, score=(train=0.985, test=0.860) total time=   3.8s\n",
      "[CV 3/5] END criterion=log_loss, max_depth=1000, n_estimators=100;, score=(train=0.985, test=0.843) total time=   0.4s\n",
      "[CV 5/5] END criterion=log_loss, max_depth=1000, n_estimators=1000;, score=(train=0.982, test=0.843) total time=   3.5s\n",
      "[CV 2/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=newton-cholesky;, score=(train=0.795, test=0.764) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=sag;, score=(train=0.789, test=0.743) total time=   0.8s\n",
      "[CV 1/5] END class_weight=None, max_iter=10000, penalty=l2, solver=newton-cholesky;, score=(train=0.806, test=0.782) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, max_iter=10000, penalty=l2, solver=newton-cholesky;, score=(train=0.799, test=0.781) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, max_iter=10000, penalty=l2, solver=newton-cholesky;, score=(train=0.797, test=0.820) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, max_iter=10000, penalty=l2, solver=sag;, score=(train=0.792, test=0.765) total time=   0.8s\n",
      "[CV 4/5] END class_weight=None, max_iter=10000, penalty=None, solver=saga;, score=(train=0.804, test=0.775) total time=   0.9s\n",
      "[CV 1/5] END learning_rate=0.01, loss=exponential, n_estimators=100;, score=(train=0.831, test=0.810) total time=   0.3s\n",
      "[CV 3/5] END learning_rate=0.1, loss=log_loss, n_estimators=10;, score=(train=0.839, test=0.826) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, loss=log_loss, n_estimators=10;, score=(train=0.844, test=0.787) total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.1, loss=log_loss, n_estimators=1000;, score=(train=0.978, test=0.820) total time=   2.4s\n",
      "[CV 1/5] END learning_rate=0.1, loss=exponential, n_estimators=10;, score=(train=0.831, test=0.810) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=10, loss=log_loss, n_estimators=10;, score=(train=0.218, test=0.196) total time=   0.0s\n",
      "[CV 2/5] END learning_rate=10, loss=log_loss, n_estimators=10;, score=(train=0.261, test=0.292) total time=   0.0s\n",
      "[CV 3/5] END learning_rate=10, loss=log_loss, n_estimators=10;, score=(train=0.613, test=0.601) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=10, loss=log_loss, n_estimators=10;, score=(train=0.236, test=0.236) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=10, loss=log_loss, n_estimators=100;, score=(train=0.236, test=0.236) total time=   0.2s\n",
      "[CV 5/5] END learning_rate=10, loss=log_loss, n_estimators=100;, score=(train=0.631, test=0.646) total time=   0.2s\n",
      "[CV 4/5] END learning_rate=10, loss=exponential, n_estimators=10;, score=(train=0.205, test=0.247) total time=   0.0s\n",
      "[CV 5/5] END learning_rate=10, loss=exponential, n_estimators=10;, score=(train=0.735, test=0.770) total time=   0.0s\n",
      "[CV 3/5] END learning_rate=10, loss=exponential, n_estimators=100;, score=(train=0.351, test=0.315) total time=   0.2s\n",
      "[CV 4/5] END learning_rate=10, loss=exponential, n_estimators=100;, score=(train=0.205, test=0.247) total time=   0.2s\n",
      "[CV 4/5] END learning_rate=10, loss=exponential, n_estimators=1000;, score=(train=0.245, test=0.275) total time=   2.3s\n",
      "[CV 5/5] END learning_rate=10, loss=exponential, n_estimators=1000;, score=(train=0.738, test=0.775) total time=   1.9s\n",
      "[CV 2/5] END learning_rate=0.001, loss=log_loss, n_estimators=10;, score=(train=0.616, test=0.618) total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.001, loss=log_loss, n_estimators=10;, score=(train=0.617, test=0.612) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.001, loss=log_loss, n_estimators=100;, score=(train=0.616, test=0.618) total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.001, loss=exponential, n_estimators=10;, score=(train=0.617, test=0.612) total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.001, loss=exponential, n_estimators=1000;, score=(train=0.836, test=0.820) total time=   2.3s\n",
      "[CV 2/5] END learning_rate=0.1, loss=exponential, n_estimators=10;, score=(train=0.832, test=0.815) total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, loss=exponential, n_estimators=10;, score=(train=0.836, test=0.820) total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, loss=exponential, n_estimators=1000;, score=(train=0.985, test=0.809) total time=   2.3s\n",
      "[CV 3/5] END learning_rate=0.1, loss=exponential, n_estimators=1000;, score=(train=0.983, test=0.826) total time=   2.2s\n",
      "[CV 4/5] END class_weight=None, criterion=log_loss, max_depth=None, splitter=best;, score=(train=0.985, test=0.758) total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=1000, n_estimators=1000;, score=(train=0.985, test=0.854) total time=   3.7s\n",
      "[CV 2/5] END criterion=log_loss, max_depth=1000, n_estimators=100;, score=(train=0.987, test=0.803) total time=   0.4s\n",
      "[CV 2/5] END criterion=log_loss, max_depth=None, n_estimators=10;, score=(train=0.968, test=0.809) total time=   0.1s\n",
      "[CV 1/5] END criterion=log_loss, max_depth=None, n_estimators=100;, score=(train=0.985, test=0.777) total time=   0.4s\n",
      "[CV 2/5] END criterion=log_loss, max_depth=None, n_estimators=1000;, score=(train=0.987, test=0.815) total time=   3.6s\n",
      "[CV 4/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=newton-cholesky;, score=(train=0.798, test=0.775) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=saga;, score=(train=0.788, test=0.749) total time=   1.0s\n",
      "[CV 4/5] END class_weight=None, max_iter=10000, penalty=None, solver=lbfgs;, score=(train=0.806, test=0.770) total time=   0.2s\n",
      "[CV 5/5] END class_weight=None, max_iter=10000, penalty=None, solver=lbfgs;, score=(train=0.802, test=0.826) total time=   0.1s\n",
      "[CV 5/5] END class_weight=None, max_iter=10000, penalty=None, solver=sag;, score=(train=0.788, test=0.809) total time=   0.8s\n",
      "[CV 1/5] END class_weight=None, max_iter=10000, penalty=None, solver=saga;, score=(train=0.799, test=0.754) total time=   0.7s\n",
      "[CV 4/5] END learning_rate=0.001, loss=log_loss, n_estimators=10;, score=(train=0.616, test=0.618) total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.001, loss=log_loss, n_estimators=100;, score=(train=0.616, test=0.618) total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.001, loss=exponential, n_estimators=1000;, score=(train=0.842, test=0.781) total time=   2.4s\n",
      "[CV 3/5] END learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=0.881, test=0.837) total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=0.900, test=0.798) total time=   0.2s\n",
      "[CV 3/5] END learning_rate=1, loss=log_loss, n_estimators=1000;, score=(train=0.985, test=0.837) total time=   2.2s\n",
      "[CV 4/5] END learning_rate=1, loss=log_loss, n_estimators=1000;, score=(train=0.985, test=0.792) total time=   2.3s\n",
      "[CV 1/5] END class_weight=balanced, criterion=gini, max_depth=100, splitter=best;, score=(train=0.985, test=0.715) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=gini, max_depth=1000, splitter=random;, score=(train=0.987, test=0.730) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=gini, max_depth=1000, splitter=random;, score=(train=0.985, test=0.809) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=gini, max_depth=1000, splitter=random;, score=(train=0.985, test=0.764) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=gini, max_depth=1000, splitter=random;, score=(train=0.982, test=0.787) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=gini, max_depth=None, splitter=best;, score=(train=0.985, test=0.709) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=gini, max_depth=None, splitter=best;, score=(train=0.985, test=0.826) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=gini, max_depth=None, splitter=best;, score=(train=0.982, test=0.792) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=gini, max_depth=None, splitter=random;, score=(train=0.987, test=0.758) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=gini, max_depth=None, splitter=random;, score=(train=0.985, test=0.725) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=entropy, max_depth=100, splitter=best;, score=(train=0.985, test=0.726) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=entropy, max_depth=100, splitter=best;, score=(train=0.985, test=0.815) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=entropy, max_depth=100, splitter=random;, score=(train=0.985, test=0.749) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=entropy, max_depth=100, splitter=random;, score=(train=0.985, test=0.775) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=entropy, max_depth=1000, splitter=best;, score=(train=0.987, test=0.770) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=entropy, max_depth=1000, splitter=best;, score=(train=0.982, test=0.781) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=entropy, max_depth=1000, splitter=random;, score=(train=0.985, test=0.764) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=entropy, max_depth=None, splitter=best;, score=(train=0.985, test=0.809) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=entropy, max_depth=None, splitter=random;, score=(train=0.987, test=0.753) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=log_loss, max_depth=100, splitter=best;, score=(train=0.985, test=0.737) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=log_loss, max_depth=100, splitter=best;, score=(train=0.982, test=0.787) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=log_loss, max_depth=1000, splitter=best;, score=(train=0.985, test=0.715) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=log_loss, max_depth=1000, splitter=random;, score=(train=0.985, test=0.754) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=log_loss, max_depth=None, splitter=best;, score=(train=0.987, test=0.764) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=log_loss, max_depth=None, splitter=random;, score=(train=0.982, test=0.803) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=gini, max_depth=100, splitter=random;, score=(train=0.987, test=0.730) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=gini, max_depth=1000, splitter=random;, score=(train=0.985, test=0.715) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=gini, max_depth=None, splitter=best;, score=(train=0.985, test=0.803) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=gini, max_depth=None, splitter=best;, score=(train=0.985, test=0.753) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=entropy, max_depth=1000, splitter=best;, score=(train=0.985, test=0.737) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=entropy, max_depth=1000, splitter=best;, score=(train=0.987, test=0.775) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=entropy, max_depth=None, splitter=random;, score=(train=0.987, test=0.770) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=entropy, max_depth=None, splitter=random;, score=(train=0.985, test=0.831) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=log_loss, max_depth=1000, splitter=best;, score=(train=0.982, test=0.792) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=log_loss, max_depth=1000, splitter=random;, score=(train=0.985, test=0.749) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=log_loss, max_depth=None, splitter=random;, score=(train=0.985, test=0.787) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=gini, max_depth=100, splitter=best;, score=(train=0.985, test=0.726) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=gini, max_depth=100, splitter=best;, score=(train=0.987, test=0.798) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=entropy, max_depth=None, splitter=random;, score=(train=0.985, test=0.781) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=entropy, max_depth=None, splitter=random;, score=(train=0.982, test=0.770) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=log_loss, max_depth=100, splitter=best;, score=(train=0.985, test=0.715) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=log_loss, max_depth=100, splitter=best;, score=(train=0.987, test=0.775) total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=1000, n_estimators=100;, score=(train=0.987, test=0.815) total time=   0.4s\n",
      "[CV 2/5] END criterion=entropy, max_depth=1000, n_estimators=10;, score=(train=0.976, test=0.798) total time=   0.0s\n",
      "[CV 2/5] END criterion=entropy, max_depth=1000, n_estimators=100;, score=(train=0.987, test=0.809) total time=   0.4s\n",
      "[CV 3/5] END criterion=entropy, max_depth=1000, n_estimators=1000;, score=(train=0.985, test=0.848) total time=   3.9s\n",
      "[CV 4/5] END criterion=log_loss, max_depth=None, n_estimators=100;, score=(train=0.985, test=0.775) total time=   0.4s\n",
      "[CV 5/5] END criterion=log_loss, max_depth=None, n_estimators=1000;, score=(train=0.982, test=0.831) total time=   3.2s\n",
      "[CV 3/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=newton-cholesky;, score=(train=0.795, test=0.787) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=sag;, score=(train=0.784, test=0.809) total time=   0.8s\n",
      "[CV 4/5] END class_weight=None, max_iter=10000, penalty=l2, solver=newton-cg;, score=(train=0.806, test=0.770) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, max_iter=10000, penalty=l2, solver=newton-cg;, score=(train=0.797, test=0.820) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, max_iter=10000, penalty=l2, solver=saga;, score=(train=0.799, test=0.749) total time=   1.1s\n",
      "[CV 2/5] END class_weight=None, max_iter=10000, penalty=l2, solver=saga;, score=(train=0.777, test=0.787) total time=   0.8s\n",
      "[CV 1/5] END learning_rate=0.01, loss=log_loss, n_estimators=100;, score=(train=0.833, test=0.810) total time=   0.3s\n",
      "[CV 4/5] END learning_rate=0.01, loss=exponential, n_estimators=1000;, score=(train=0.896, test=0.787) total time=   2.4s\n",
      "[CV 5/5] END learning_rate=1, loss=exponential, n_estimators=100;, score=(train=0.979, test=0.826) total time=   0.3s\n",
      "[CV 1/5] END learning_rate=1, loss=exponential, n_estimators=1000;, score=(train=0.985, test=0.793) total time=   2.3s\n",
      "[CV 5/5] END learning_rate=100, loss=log_loss, n_estimators=100;, score=(train=0.655, test=0.680) total time=   0.2s\n",
      "[CV 4/5] END learning_rate=100, loss=log_loss, n_estimators=1000;, score=(train=0.293, test=0.292) total time=   2.3s\n",
      "[CV 2/5] END learning_rate=0.001, loss=log_loss, n_estimators=100;, score=(train=0.616, test=0.618) total time=   0.1s\n",
      "[CV 1/5] END learning_rate=0.001, loss=exponential, n_estimators=10;, score=(train=0.617, test=0.615) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.001, loss=exponential, n_estimators=100;, score=(train=0.616, test=0.618) total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.01, loss=log_loss, n_estimators=1000;, score=(train=0.903, test=0.848) total time=   2.2s\n",
      "[CV 1/5] END learning_rate=0.01, loss=exponential, n_estimators=10;, score=(train=0.617, test=0.615) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1, loss=log_loss, n_estimators=10;, score=(train=0.899, test=0.771) total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1, loss=log_loss, n_estimators=10;, score=(train=0.905, test=0.826) total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1, loss=log_loss, n_estimators=10;, score=(train=0.891, test=0.848) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1, loss=log_loss, n_estimators=10;, score=(train=0.892, test=0.798) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1, loss=log_loss, n_estimators=100;, score=(train=0.985, test=0.787) total time=   0.2s\n",
      "[CV 5/5] END learning_rate=1, loss=log_loss, n_estimators=100;, score=(train=0.982, test=0.815) total time=   0.2s\n",
      "[CV 4/5] END learning_rate=1, loss=exponential, n_estimators=10;, score=(train=0.874, test=0.820) total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1, loss=exponential, n_estimators=10;, score=(train=0.875, test=0.860) total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1, loss=exponential, n_estimators=100;, score=(train=0.985, test=0.831) total time=   0.2s\n",
      "[CV 4/5] END learning_rate=1, loss=exponential, n_estimators=100;, score=(train=0.983, test=0.770) total time=   0.2s\n",
      "[CV 4/5] END learning_rate=1, loss=exponential, n_estimators=1000;, score=(train=0.985, test=0.764) total time=   2.3s\n",
      "[CV 5/5] END learning_rate=1, loss=exponential, n_estimators=1000;, score=(train=0.982, test=0.798) total time=   1.8s\n",
      "[CV 2/5] END class_weight=balanced, criterion=gini, max_depth=100, splitter=best;, score=(train=0.987, test=0.798) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=gini, max_depth=100, splitter=random;, score=(train=0.985, test=0.809) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=gini, max_depth=1000, splitter=best;, score=(train=0.982, test=0.798) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=gini, max_depth=None, splitter=random;, score=(train=0.985, test=0.709) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=entropy, max_depth=100, splitter=best;, score=(train=0.985, test=0.715) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=entropy, max_depth=1000, splitter=best;, score=(train=0.982, test=0.775) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=entropy, max_depth=1000, splitter=random;, score=(train=0.985, test=0.726) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=entropy, max_depth=None, splitter=best;, score=(train=0.985, test=0.826) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=entropy, max_depth=None, splitter=best;, score=(train=0.985, test=0.725) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=log_loss, max_depth=1000, splitter=best;, score=(train=0.982, test=0.781) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=log_loss, max_depth=1000, splitter=random;, score=(train=0.985, test=0.732) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=gini, max_depth=100, splitter=best;, score=(train=0.985, test=0.781) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=gini, max_depth=100, splitter=best;, score=(train=0.985, test=0.742) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=entropy, max_depth=1000, splitter=best;, score=(train=0.985, test=0.798) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=entropy, max_depth=1000, splitter=best;, score=(train=0.985, test=0.753) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=entropy, max_depth=1000, splitter=best;, score=(train=0.982, test=0.787) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=entropy, max_depth=1000, splitter=random;, score=(train=0.985, test=0.793) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=log_loss, max_depth=None, splitter=best;, score=(train=0.987, test=0.770) total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=1000, n_estimators=10;, score=(train=0.966, test=0.815) total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=1000, n_estimators=1000;, score=(train=0.985, test=0.788) total time=   3.6s\n",
      "[CV 3/5] END criterion=log_loss, max_depth=1000, n_estimators=10;, score=(train=0.964, test=0.843) total time=   0.0s\n",
      "[CV 4/5] END criterion=log_loss, max_depth=1000, n_estimators=10;, score=(train=0.969, test=0.775) total time=   0.1s\n",
      "[CV 3/5] END criterion=log_loss, max_depth=1000, n_estimators=100;, score=(train=0.985, test=0.848) total time=   0.5s\n",
      "[CV 5/5] END criterion=log_loss, max_depth=None, n_estimators=100;, score=(train=0.982, test=0.843) total time=   0.4s\n",
      "[CV 1/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=lbfgs;, score=(train=0.805, test=0.771) total time=   0.1s\n",
      "[CV 1/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=newton-cg;, score=(train=0.805, test=0.777) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=newton-cholesky;, score=(train=0.795, test=0.764) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=sag;, score=(train=0.784, test=0.809) total time=   0.7s\n",
      "[CV 5/5] END class_weight=None, max_iter=10000, penalty=l2, solver=lbfgs;, score=(train=0.797, test=0.820) total time=   0.1s\n",
      "[CV 1/5] END class_weight=None, max_iter=10000, penalty=l2, solver=newton-cg;, score=(train=0.806, test=0.782) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, max_iter=10000, penalty=l2, solver=saga;, score=(train=0.797, test=0.792) total time=   1.1s\n",
      "[CV 4/5] END class_weight=None, max_iter=10000, penalty=l2, solver=saga;, score=(train=0.806, test=0.770) total time=   0.8s\n",
      "[CV 1/5] END learning_rate=0.001, loss=exponential, n_estimators=10;, score=(train=0.617, test=0.615) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.001, loss=exponential, n_estimators=10;, score=(train=0.616, test=0.618) total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.001, loss=exponential, n_estimators=100;, score=(train=0.617, test=0.612) total time=   0.3s\n",
      "[CV 5/5] END learning_rate=0.01, loss=log_loss, n_estimators=1000;, score=(train=0.903, test=0.848) total time=   2.2s\n",
      "[CV 1/5] END learning_rate=0.01, loss=exponential, n_estimators=10;, score=(train=0.617, test=0.615) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1, loss=log_loss, n_estimators=10;, score=(train=0.899, test=0.771) total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1, loss=log_loss, n_estimators=10;, score=(train=0.905, test=0.826) total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1, loss=log_loss, n_estimators=10;, score=(train=0.891, test=0.848) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1, loss=log_loss, n_estimators=10;, score=(train=0.892, test=0.792) total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1, loss=log_loss, n_estimators=10;, score=(train=0.899, test=0.831) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1, loss=log_loss, n_estimators=100;, score=(train=0.985, test=0.788) total time=   0.2s\n",
      "[CV 5/5] END learning_rate=1, loss=log_loss, n_estimators=1000;, score=(train=0.982, test=0.798) total time=   2.2s\n",
      "[CV 1/5] END learning_rate=1, loss=exponential, n_estimators=10;, score=(train=0.878, test=0.788) total time=   0.0s\n",
      "[CV 5/5] END learning_rate=10, loss=log_loss, n_estimators=1000;, score=(train=0.631, test=0.646) total time=   2.2s\n",
      "[CV 1/5] END class_weight=balanced, criterion=gini, max_depth=1000, splitter=best;, score=(train=0.985, test=0.726) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=entropy, max_depth=None, splitter=best;, score=(train=0.982, test=0.775) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=entropy, max_depth=None, splitter=random;, score=(train=0.985, test=0.737) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=log_loss, max_depth=None, splitter=best;, score=(train=0.985, test=0.809) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=log_loss, max_depth=None, splitter=best;, score=(train=0.985, test=0.764) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=entropy, max_depth=1000, splitter=best;, score=(train=0.985, test=0.787) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=entropy, max_depth=1000, splitter=best;, score=(train=0.985, test=0.764) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=entropy, max_depth=1000, splitter=best;, score=(train=0.982, test=0.792) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=entropy, max_depth=1000, splitter=random;, score=(train=0.985, test=0.715) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=log_loss, max_depth=None, splitter=best;, score=(train=0.985, test=0.787) total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=1000, n_estimators=1000;, score=(train=0.982, test=0.843) total time=   3.8s\n",
      "[CV 2/5] END criterion=log_loss, max_depth=1000, n_estimators=10;, score=(train=0.966, test=0.792) total time=   0.0s\n",
      "[CV 1/5] END criterion=log_loss, max_depth=1000, n_estimators=100;, score=(train=0.985, test=0.777) total time=   0.4s\n",
      "[CV 3/5] END criterion=log_loss, max_depth=1000, n_estimators=1000;, score=(train=0.985, test=0.848) total time=   3.5s\n",
      "[CV 1/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=newton-cholesky;, score=(train=0.805, test=0.777) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=newton-cholesky;, score=(train=0.788, test=0.787) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=sag;, score=(train=0.778, test=0.803) total time=   0.7s\n",
      "[CV 2/5] END class_weight=None, max_iter=10000, penalty=l2, solver=newton-cg;, score=(train=0.799, test=0.781) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, max_iter=10000, penalty=l2, solver=newton-cg;, score=(train=0.801, test=0.781) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, max_iter=10000, penalty=l2, solver=sag;, score=(train=0.799, test=0.764) total time=   0.7s\n",
      "[CV 5/5] END class_weight=None, max_iter=10000, penalty=l2, solver=sag;, score=(train=0.791, test=0.815) total time=   0.7s\n",
      "[CV 5/5] END learning_rate=0.01, loss=exponential, n_estimators=10;, score=(train=0.617, test=0.612) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.01, loss=exponential, n_estimators=1000;, score=(train=0.900, test=0.816) total time=   2.5s\n",
      "[CV 5/5] END learning_rate=1, loss=log_loss, n_estimators=1000;, score=(train=0.982, test=0.820) total time=   2.3s\n",
      "[CV 1/5] END learning_rate=1, loss=exponential, n_estimators=10;, score=(train=0.878, test=0.788) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=100, loss=log_loss, n_estimators=10;, score=(train=0.301, test=0.341) total time=   0.0s\n",
      "[CV 2/5] END learning_rate=100, loss=log_loss, n_estimators=10;, score=(train=0.309, test=0.326) total time=   0.0s\n",
      "[CV 3/5] END learning_rate=100, loss=log_loss, n_estimators=10;, score=(train=0.398, test=0.382) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=100, loss=log_loss, n_estimators=10;, score=(train=0.293, test=0.292) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=100, loss=log_loss, n_estimators=100;, score=(train=0.293, test=0.292) total time=   0.2s\n",
      "[CV 1/5] END learning_rate=100, loss=log_loss, n_estimators=1000;, score=(train=0.301, test=0.341) total time=   2.4s\n",
      "[CV 1/5] END learning_rate=0.001, loss=exponential, n_estimators=100;, score=(train=0.617, test=0.615) total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.01, loss=log_loss, n_estimators=100;, score=(train=0.832, test=0.815) total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.01, loss=log_loss, n_estimators=100;, score=(train=0.839, test=0.826) total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.01, loss=exponential, n_estimators=100;, score=(train=0.835, test=0.848) total time=   0.2s\n",
      "[CV 1/5] END learning_rate=0.01, loss=exponential, n_estimators=1000;, score=(train=0.900, test=0.816) total time=   2.3s\n",
      "[CV 5/5] END learning_rate=1, loss=exponential, n_estimators=100;, score=(train=0.979, test=0.826) total time=   0.2s\n",
      "[CV 1/5] END learning_rate=1, loss=exponential, n_estimators=1000;, score=(train=0.985, test=0.793) total time=   2.3s\n",
      "[CV 1/5] END learning_rate=10, loss=exponential, n_estimators=100;, score=(train=0.256, test=0.218) total time=   0.2s\n",
      "[CV 4/5] END learning_rate=10, loss=exponential, n_estimators=100;, score=(train=0.205, test=0.247) total time=   0.2s\n",
      "[CV 2/5] END learning_rate=10, loss=exponential, n_estimators=1000;, score=(train=0.506, test=0.461) total time=   1.7s\n",
      "[CV 3/5] END class_weight=balanced, criterion=gini, max_depth=1000, splitter=best;, score=(train=0.985, test=0.826) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=gini, max_depth=1000, splitter=random;, score=(train=0.982, test=0.781) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=gini, max_depth=None, splitter=random;, score=(train=0.985, test=0.837) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=entropy, max_depth=100, splitter=best;, score=(train=0.985, test=0.837) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=entropy, max_depth=100, splitter=best;, score=(train=0.985, test=0.764) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=entropy, max_depth=None, splitter=best;, score=(train=0.982, test=0.787) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=entropy, max_depth=None, splitter=random;, score=(train=0.985, test=0.771) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=log_loss, max_depth=1000, splitter=best;, score=(train=0.985, test=0.726) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=log_loss, max_depth=1000, splitter=best;, score=(train=0.987, test=0.781) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=gini, max_depth=100, splitter=best;, score=(train=0.982, test=0.803) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=gini, max_depth=100, splitter=random;, score=(train=0.985, test=0.771) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=entropy, max_depth=1000, splitter=random;, score=(train=0.987, test=0.764) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=entropy, max_depth=1000, splitter=random;, score=(train=0.985, test=0.809) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=entropy, max_depth=1000, splitter=random;, score=(train=0.985, test=0.798) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=entropy, max_depth=1000, splitter=random;, score=(train=0.982, test=0.798) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=log_loss, max_depth=1000, splitter=random;, score=(train=0.982, test=0.798) total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=1000, n_estimators=10;, score=(train=0.966, test=0.782) total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=1000, n_estimators=100;, score=(train=0.985, test=0.854) total time=   0.3s\n",
      "[CV 4/5] END criterion=gini, max_depth=None, n_estimators=1000;, score=(train=0.985, test=0.770) total time=   3.8s\n",
      "[CV 4/5] END criterion=log_loss, max_depth=1000, n_estimators=1000;, score=(train=0.985, test=0.770) total time=   3.7s\n",
      "[CV 2/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=lbfgs;, score=(train=0.792, test=0.764) total time=   0.1s\n",
      "[CV 3/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=newton-cg;, score=(train=0.794, test=0.787) total time=   0.1s\n",
      "[CV 4/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=newton-cg;, score=(train=0.802, test=0.775) total time=   0.1s\n",
      "[CV 1/5] END class_weight=None, max_iter=10000, penalty=l2, solver=lbfgs;, score=(train=0.806, test=0.782) total time=   0.1s\n",
      "[CV 2/5] END class_weight=None, max_iter=10000, penalty=l2, solver=lbfgs;, score=(train=0.799, test=0.781) total time=   0.1s\n",
      "[CV 1/5] END class_weight=None, max_iter=10000, penalty=l2, solver=newton-cholesky;, score=(train=0.806, test=0.782) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, max_iter=10000, penalty=l2, solver=newton-cholesky;, score=(train=0.799, test=0.781) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, max_iter=10000, penalty=l2, solver=sag;, score=(train=0.791, test=0.798) total time=   0.8s\n",
      "[CV 3/5] END class_weight=None, max_iter=10000, penalty=l2, solver=sag;, score=(train=0.802, test=0.781) total time=   0.7s\n",
      "[CV 3/5] END learning_rate=0.001, loss=log_loss, n_estimators=1000;, score=(train=0.839, test=0.826) total time=   2.3s\n",
      "[CV 2/5] END learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.914, test=0.809) total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.893, test=0.837) total time=   0.2s\n",
      "[CV 2/5] END learning_rate=1, loss=log_loss, n_estimators=100;, score=(train=0.987, test=0.781) total time=   0.2s\n",
      "[CV 3/5] END learning_rate=1, loss=log_loss, n_estimators=100;, score=(train=0.985, test=0.831) total time=   0.2s\n",
      "[CV 2/5] END learning_rate=1, loss=exponential, n_estimators=10;, score=(train=0.879, test=0.809) total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1, loss=exponential, n_estimators=10;, score=(train=0.877, test=0.831) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1, loss=exponential, n_estimators=100;, score=(train=0.983, test=0.788) total time=   0.2s\n",
      "[CV 2/5] END learning_rate=1, loss=exponential, n_estimators=100;, score=(train=0.985, test=0.820) total time=   0.2s\n",
      "[CV 2/5] END learning_rate=1, loss=exponential, n_estimators=1000;, score=(train=0.987, test=0.781) total time=   2.2s\n",
      "[CV 3/5] END learning_rate=1, loss=exponential, n_estimators=1000;, score=(train=0.985, test=0.826) total time=   2.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=gini, max_depth=100, splitter=random;, score=(train=0.985, test=0.721) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=log_loss, max_depth=None, splitter=best;, score=(train=0.985, test=0.732) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=log_loss, max_depth=None, splitter=random;, score=(train=0.987, test=0.742) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=gini, max_depth=100, splitter=best;, score=(train=0.985, test=0.742) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=gini, max_depth=1000, splitter=best;, score=(train=0.985, test=0.798) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=gini, max_depth=1000, splitter=random;, score=(train=0.982, test=0.815) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=entropy, max_depth=100, splitter=best;, score=(train=0.985, test=0.760) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=entropy, max_depth=100, splitter=best;, score=(train=0.987, test=0.781) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=entropy, max_depth=1000, splitter=random;, score=(train=0.987, test=0.758) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=entropy, max_depth=1000, splitter=random;, score=(train=0.985, test=0.764) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=log_loss, max_depth=100, splitter=best;, score=(train=0.982, test=0.792) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=log_loss, max_depth=100, splitter=random;, score=(train=0.985, test=0.754) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=log_loss, max_depth=1000, splitter=random;, score=(train=0.985, test=0.758) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=log_loss, max_depth=1000, splitter=random;, score=(train=0.982, test=0.803) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=gini, max_depth=100, splitter=random;, score=(train=0.987, test=0.770) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=gini, max_depth=1000, splitter=best;, score=(train=0.985, test=0.725) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=gini, max_depth=None, splitter=best;, score=(train=0.985, test=0.730) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=entropy, max_depth=1000, splitter=best;, score=(train=0.982, test=0.787) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=entropy, max_depth=1000, splitter=random;, score=(train=0.985, test=0.782) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=log_loss, max_depth=None, splitter=best;, score=(train=0.982, test=0.764) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=log_loss, max_depth=None, splitter=random;, score=(train=0.985, test=0.749) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=entropy, max_depth=100, splitter=random;, score=(train=0.985, test=0.781) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=entropy, max_depth=100, splitter=random;, score=(train=0.982, test=0.803) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=entropy, max_depth=1000, splitter=best;, score=(train=0.985, test=0.721) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=entropy, max_depth=1000, splitter=best;, score=(train=0.987, test=0.781) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=log_loss, max_depth=1000, splitter=random;, score=(train=0.985, test=0.831) total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=1000, n_estimators=1000;, score=(train=0.985, test=0.865) total time=   3.8s\n",
      "[CV 3/5] END criterion=entropy, max_depth=None, n_estimators=1000;, score=(train=0.985, test=0.848) total time=   3.7s\n",
      "[CV 3/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=newton-cg;, score=(train=0.795, test=0.787) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=sag;, score=(train=0.791, test=0.747) total time=   0.7s\n",
      "[CV 5/5] END class_weight=None, max_iter=10000, penalty=l2, solver=lbfgs;, score=(train=0.797, test=0.820) total time=   0.1s\n",
      "[CV 1/5] END class_weight=None, max_iter=10000, penalty=l2, solver=newton-cg;, score=(train=0.806, test=0.782) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, max_iter=10000, penalty=l2, solver=saga;, score=(train=0.797, test=0.792) total time=   1.1s\n",
      "[CV 4/5] END class_weight=None, max_iter=10000, penalty=l2, solver=saga;, score=(train=0.806, test=0.770) total time=   0.8s\n",
      "[CV 2/5] END learning_rate=0.01, loss=log_loss, n_estimators=100;, score=(train=0.832, test=0.815) total time=   0.3s\n",
      "[CV 3/5] END learning_rate=0.01, loss=exponential, n_estimators=1000;, score=(train=0.879, test=0.843) total time=   2.4s\n",
      "[CV 4/5] END learning_rate=1, loss=exponential, n_estimators=10;, score=(train=0.874, test=0.820) total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1, loss=exponential, n_estimators=10;, score=(train=0.875, test=0.860) total time=   0.0s\n",
      "[CV 2/5] END learning_rate=1, loss=exponential, n_estimators=1000;, score=(train=0.987, test=0.781) total time=   2.3s\n",
      "[CV 3/5] END learning_rate=1, loss=exponential, n_estimators=1000;, score=(train=0.985, test=0.826) total time=   2.4s\n",
      "[CV 1/5] END learning_rate=0.001, loss=log_loss, n_estimators=10;, score=(train=0.617, test=0.615) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.001, loss=log_loss, n_estimators=10;, score=(train=0.616, test=0.618) total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.001, loss=log_loss, n_estimators=100;, score=(train=0.616, test=0.618) total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.001, loss=exponential, n_estimators=10;, score=(train=0.616, test=0.618) total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.001, loss=exponential, n_estimators=100;, score=(train=0.616, test=0.618) total time=   0.2s\n",
      "[CV 1/5] END learning_rate=0.01, loss=log_loss, n_estimators=1000;, score=(train=0.893, test=0.793) total time=   2.2s\n",
      "[CV 2/5] END learning_rate=0.01, loss=log_loss, n_estimators=1000;, score=(train=0.910, test=0.837) total time=   2.2s\n",
      "[CV 3/5] END learning_rate=10, loss=log_loss, n_estimators=10;, score=(train=0.613, test=0.601) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=10, loss=log_loss, n_estimators=10;, score=(train=0.236, test=0.236) total time=   0.0s\n",
      "[CV 2/5] END learning_rate=10, loss=log_loss, n_estimators=100;, score=(train=0.261, test=0.292) total time=   0.2s\n",
      "[CV 3/5] END learning_rate=10, loss=log_loss, n_estimators=100;, score=(train=0.613, test=0.601) total time=   0.2s\n",
      "[CV 4/5] END learning_rate=10, loss=log_loss, n_estimators=1000;, score=(train=0.236, test=0.236) total time=   2.1s\n",
      "[CV 5/5] END class_weight=balanced, criterion=gini, max_depth=100, splitter=best;, score=(train=0.982, test=0.781) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=gini, max_depth=100, splitter=random;, score=(train=0.982, test=0.753) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=gini, max_depth=1000, splitter=random;, score=(train=0.987, test=0.764) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=gini, max_depth=None, splitter=best;, score=(train=0.985, test=0.736) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=entropy, max_depth=1000, splitter=best;, score=(train=0.985, test=0.732) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=entropy, max_depth=1000, splitter=best;, score=(train=0.987, test=0.770) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=log_loss, max_depth=100, splitter=best;, score=(train=0.985, test=0.820) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=log_loss, max_depth=100, splitter=best;, score=(train=0.985, test=0.742) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=log_loss, max_depth=None, splitter=best;, score=(train=0.985, test=0.737) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=log_loss, max_depth=None, splitter=best;, score=(train=0.987, test=0.770) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=gini, max_depth=1000, splitter=best;, score=(train=0.982, test=0.826) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=gini, max_depth=1000, splitter=random;, score=(train=0.985, test=0.777) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=gini, max_depth=1000, splitter=random;, score=(train=0.987, test=0.770) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=gini, max_depth=1000, splitter=random;, score=(train=0.985, test=0.837) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=entropy, max_depth=None, splitter=random;, score=(train=0.985, test=0.803) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=entropy, max_depth=None, splitter=random;, score=(train=0.982, test=0.803) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=log_loss, max_depth=100, splitter=best;, score=(train=0.985, test=0.754) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=log_loss, max_depth=100, splitter=best;, score=(train=0.987, test=0.781) total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=None, n_estimators=10;, score=(train=0.969, test=0.854) total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=None, n_estimators=100;, score=(train=0.985, test=0.854) total time=   0.4s\n",
      "[CV 1/5] END criterion=entropy, max_depth=1000, n_estimators=1000;, score=(train=0.985, test=0.793) total time=   3.7s\n",
      "[CV 2/5] END criterion=entropy, max_depth=1000, n_estimators=1000;, score=(train=0.987, test=0.820) total time=   3.6s\n",
      "[CV 5/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=lbfgs;, score=(train=0.787, test=0.787) total time=   0.1s\n",
      "[CV 4/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=saga;, score=(train=0.788, test=0.753) total time=   1.0s\n",
      "[CV 2/5] END class_weight=None, max_iter=10000, penalty=None, solver=lbfgs;, score=(train=0.798, test=0.781) total time=   0.2s\n",
      "[CV 3/5] END class_weight=None, max_iter=10000, penalty=None, solver=lbfgs;, score=(train=0.799, test=0.775) total time=   0.2s\n",
      "[CV 3/5] END class_weight=None, max_iter=10000, penalty=None, solver=saga;, score=(train=0.801, test=0.787) total time=   1.1s\n",
      "[CV 5/5] END learning_rate=0.001, loss=log_loss, n_estimators=1000;, score=(train=0.836, test=0.848) total time=   2.3s\n",
      "[CV 4/5] END learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.913, test=0.809) total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.905, test=0.831) total time=   0.2s\n",
      "[CV 4/5] END learning_rate=1, loss=log_loss, n_estimators=100;, score=(train=0.985, test=0.787) total time=   0.2s\n",
      "[CV 5/5] END learning_rate=1, loss=log_loss, n_estimators=100;, score=(train=0.982, test=0.792) total time=   0.2s\n",
      "[CV 4/5] END learning_rate=1, loss=exponential, n_estimators=10;, score=(train=0.874, test=0.820) total time=   0.0s\n",
      "[CV 5/5] END learning_rate=1, loss=exponential, n_estimators=10;, score=(train=0.875, test=0.860) total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1, loss=exponential, n_estimators=100;, score=(train=0.985, test=0.831) total time=   0.2s\n",
      "[CV 4/5] END learning_rate=1, loss=exponential, n_estimators=100;, score=(train=0.983, test=0.770) total time=   0.2s\n",
      "[CV 4/5] END learning_rate=1, loss=exponential, n_estimators=1000;, score=(train=0.985, test=0.758) total time=   2.3s\n",
      "[CV 5/5] END learning_rate=1, loss=exponential, n_estimators=1000;, score=(train=0.982, test=0.803) total time=   2.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=gini, max_depth=1000, splitter=best;, score=(train=0.982, test=0.803) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=gini, max_depth=100, splitter=best;, score=(train=0.982, test=0.787) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=gini, max_depth=1000, splitter=best;, score=(train=0.985, test=0.721) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=gini, max_depth=None, splitter=best;, score=(train=0.985, test=0.715) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=gini, max_depth=None, splitter=random;, score=(train=0.982, test=0.770) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=entropy, max_depth=1000, splitter=best;, score=(train=0.985, test=0.798) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=entropy, max_depth=1000, splitter=best;, score=(train=0.985, test=0.742) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=log_loss, max_depth=1000, splitter=random;, score=(train=0.985, test=0.792) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=log_loss, max_depth=1000, splitter=random;, score=(train=0.982, test=0.781) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=gini, max_depth=None, splitter=best;, score=(train=0.985, test=0.792) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=gini, max_depth=None, splitter=best;, score=(train=0.985, test=0.742) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=gini, max_depth=None, splitter=best;, score=(train=0.982, test=0.831) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=gini, max_depth=None, splitter=random;, score=(train=0.985, test=0.749) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=log_loss, max_depth=1000, splitter=random;, score=(train=0.985, test=0.726) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=log_loss, max_depth=None, splitter=random;, score=(train=0.985, test=0.732) total time=   0.0s\n",
      "[CV 3/5] END criterion=gini, max_depth=1000, n_estimators=10;, score=(train=0.965, test=0.826) total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=None, n_estimators=10;, score=(train=0.966, test=0.820) total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=None, n_estimators=100;, score=(train=0.985, test=0.758) total time=   0.3s\n",
      "[CV 1/5] END criterion=entropy, max_depth=1000, n_estimators=10;, score=(train=0.966, test=0.765) total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=1000, n_estimators=10;, score=(train=0.971, test=0.826) total time=   0.1s\n",
      "[CV 3/5] END criterion=entropy, max_depth=1000, n_estimators=100;, score=(train=0.985, test=0.848) total time=   0.4s\n",
      "[CV 4/5] END criterion=entropy, max_depth=1000, n_estimators=1000;, score=(train=0.985, test=0.775) total time=   3.9s\n",
      "[CV 1/5] END criterion=log_loss, max_depth=None, n_estimators=1000;, score=(train=0.985, test=0.788) total time=   3.4s\n",
      "[CV 4/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=lbfgs;, score=(train=0.801, test=0.775) total time=   0.2s\n",
      "[CV 2/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=newton-cholesky;, score=(train=0.794, test=0.764) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=newton-cholesky;, score=(train=0.794, test=0.787) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=sag;, score=(train=0.788, test=0.787) total time=   0.8s\n",
      "[CV 4/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=sag;, score=(train=0.791, test=0.747) total time=   0.8s\n",
      "[CV 1/5] END learning_rate=0.01, loss=log_loss, n_estimators=10;, score=(train=0.617, test=0.615) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.01, loss=log_loss, n_estimators=1000;, score=(train=0.907, test=0.803) total time=   2.4s\n",
      "[CV 4/5] END learning_rate=1, loss=log_loss, n_estimators=100;, score=(train=0.985, test=0.787) total time=   0.2s\n",
      "[CV 5/5] END learning_rate=1, loss=log_loss, n_estimators=100;, score=(train=0.982, test=0.787) total time=   0.2s\n",
      "[CV 5/5] END learning_rate=10, loss=log_loss, n_estimators=10;, score=(train=0.631, test=0.646) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=10, loss=log_loss, n_estimators=100;, score=(train=0.218, test=0.196) total time=   0.3s\n",
      "[CV 5/5] END learning_rate=10, loss=log_loss, n_estimators=1000;, score=(train=0.631, test=0.646) total time=   2.2s\n",
      "[CV 1/5] END learning_rate=10, loss=exponential, n_estimators=10;, score=(train=0.247, test=0.212) total time=   0.0s\n",
      "[CV 5/5] END learning_rate=100, loss=log_loss, n_estimators=1000;, score=(train=0.655, test=0.680) total time=   2.3s\n",
      "[CV 5/5] END learning_rate=0.001, loss=log_loss, n_estimators=100;, score=(train=0.617, test=0.612) total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.001, loss=exponential, n_estimators=100;, score=(train=0.617, test=0.612) total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.01, loss=log_loss, n_estimators=1000;, score=(train=0.898, test=0.848) total time=   2.2s\n",
      "[CV 4/5] END learning_rate=0.01, loss=log_loss, n_estimators=1000;, score=(train=0.907, test=0.803) total time=   2.2s\n",
      "[CV 4/5] END learning_rate=10, loss=log_loss, n_estimators=100;, score=(train=0.236, test=0.236) total time=   0.2s\n",
      "[CV 2/5] END learning_rate=10, loss=log_loss, n_estimators=1000;, score=(train=0.261, test=0.292) total time=   2.2s\n",
      "[CV 1/5] END class_weight=None, criterion=entropy, max_depth=None, splitter=best;, score=(train=0.985, test=0.726) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=entropy, max_depth=None, splitter=best;, score=(train=0.987, test=0.787) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=entropy, max_depth=None, splitter=best;, score=(train=0.985, test=0.787) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=entropy, max_depth=None, splitter=best;, score=(train=0.985, test=0.764) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=log_loss, max_depth=None, splitter=best;, score=(train=0.982, test=0.792) total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=None, n_estimators=10;, score=(train=0.966, test=0.770) total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=None, n_estimators=100;, score=(train=0.985, test=0.777) total time=   0.4s\n",
      "[CV 5/5] END criterion=entropy, max_depth=1000, n_estimators=10;, score=(train=0.965, test=0.820) total time=   0.0s\n",
      "[CV 1/5] END criterion=entropy, max_depth=1000, n_estimators=100;, score=(train=0.985, test=0.782) total time=   0.4s\n",
      "[CV 2/5] END criterion=entropy, max_depth=None, n_estimators=10;, score=(train=0.968, test=0.809) total time=   0.0s\n",
      "[CV 3/5] END criterion=entropy, max_depth=None, n_estimators=10;, score=(train=0.972, test=0.837) total time=   0.0s\n",
      "[CV 4/5] END criterion=entropy, max_depth=None, n_estimators=10;, score=(train=0.971, test=0.792) total time=   0.1s\n",
      "[CV 5/5] END criterion=entropy, max_depth=None, n_estimators=10;, score=(train=0.964, test=0.865) total time=   0.1s\n",
      "[CV 1/5] END criterion=entropy, max_depth=None, n_estimators=100;, score=(train=0.985, test=0.771) total time=   0.4s\n",
      "[CV 2/5] END criterion=entropy, max_depth=None, n_estimators=100;, score=(train=0.987, test=0.803) total time=   0.4s\n",
      "[CV 2/5] END criterion=entropy, max_depth=None, n_estimators=1000;, score=(train=0.987, test=0.815) total time=   3.8s\n",
      "[CV 3/5] END criterion=entropy, max_depth=None, n_estimators=1000;, score=(train=0.985, test=0.848) total time=   3.1s\n",
      "[CV 3/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=lbfgs;, score=(train=0.795, test=0.787) total time=   0.1s\n",
      "[CV 4/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=newton-cg;, score=(train=0.798, test=0.775) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=sag;, score=(train=0.791, test=0.747) total time=   0.7s\n",
      "[CV 3/5] END class_weight=None, max_iter=10000, penalty=l2, solver=lbfgs;, score=(train=0.801, test=0.781) total time=   0.1s\n",
      "[CV 4/5] END class_weight=None, max_iter=10000, penalty=l2, solver=lbfgs;, score=(train=0.806, test=0.770) total time=   0.1s\n",
      "[CV 5/5] END class_weight=None, max_iter=10000, penalty=l2, solver=saga;, score=(train=0.795, test=0.815) total time=   1.1s\n",
      "[CV 1/5] END class_weight=None, max_iter=10000, penalty=None, solver=lbfgs;, score=(train=0.806, test=0.782) total time=   0.1s\n",
      "[CV 1/5] END learning_rate=0.001, loss=exponential, n_estimators=100;, score=(train=0.617, test=0.615) total time=   0.3s\n",
      "[CV 2/5] END learning_rate=0.01, loss=log_loss, n_estimators=100;, score=(train=0.832, test=0.815) total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.01, loss=log_loss, n_estimators=100;, score=(train=0.839, test=0.826) total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.01, loss=exponential, n_estimators=100;, score=(train=0.835, test=0.848) total time=   0.3s\n",
      "[CV 1/5] END learning_rate=0.01, loss=exponential, n_estimators=1000;, score=(train=0.900, test=0.816) total time=   2.3s\n",
      "[CV 5/5] END learning_rate=1, loss=exponential, n_estimators=100;, score=(train=0.979, test=0.826) total time=   0.2s\n",
      "[CV 1/5] END learning_rate=1, loss=exponential, n_estimators=1000;, score=(train=0.985, test=0.793) total time=   2.3s\n",
      "[CV 3/5] END learning_rate=10, loss=exponential, n_estimators=100;, score=(train=0.351, test=0.315) total time=   0.2s\n",
      "[CV 1/5] END learning_rate=10, loss=exponential, n_estimators=1000;, score=(train=0.496, test=0.458) total time=   2.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=gini, max_depth=1000, splitter=best;, score=(train=0.985, test=0.820) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=gini, max_depth=100, splitter=best;, score=(train=0.987, test=0.798) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=gini, max_depth=100, splitter=random;, score=(train=0.985, test=0.792) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=gini, max_depth=1000, splitter=random;, score=(train=0.985, test=0.704) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=gini, max_depth=None, splitter=best;, score=(train=0.982, test=0.787) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=entropy, max_depth=100, splitter=random;, score=(train=0.987, test=0.764) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=entropy, max_depth=100, splitter=random;, score=(train=0.985, test=0.826) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=gini, max_depth=100, splitter=best;, score=(train=0.985, test=0.815) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=gini, max_depth=100, splitter=best;, score=(train=0.985, test=0.747) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=entropy, max_depth=None, splitter=best;, score=(train=0.985, test=0.754) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=entropy, max_depth=None, splitter=best;, score=(train=0.987, test=0.787) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=entropy, max_depth=None, splitter=best;, score=(train=0.985, test=0.787) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=entropy, max_depth=None, splitter=best;, score=(train=0.985, test=0.753) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=log_loss, max_depth=None, splitter=random;, score=(train=0.987, test=0.764) total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=None, n_estimators=10;, score=(train=0.963, test=0.804) total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=None, n_estimators=1000;, score=(train=0.985, test=0.788) total time=   3.8s\n",
      "[CV 5/5] END criterion=log_loss, max_depth=1000, n_estimators=10;, score=(train=0.961, test=0.843) total time=   0.0s\n",
      "[CV 1/5] END criterion=log_loss, max_depth=1000, n_estimators=1000;, score=(train=0.985, test=0.782) total time=   3.6s\n",
      "[CV 4/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=lbfgs;, score=(train=0.798, test=0.775) total time=   0.1s\n",
      "[CV 5/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=lbfgs;, score=(train=0.788, test=0.787) total time=   0.2s\n",
      "[CV 5/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=newton-cg;, score=(train=0.787, test=0.787) total time=   0.1s\n",
      "[CV 1/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=newton-cholesky;, score=(train=0.805, test=0.765) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=saga;, score=(train=0.792, test=0.753) total time=   1.1s\n",
      "[CV 5/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=saga;, score=(train=0.780, test=0.803) total time=   1.0s\n",
      "[CV 3/5] END learning_rate=0.01, loss=log_loss, n_estimators=10;, score=(train=0.616, test=0.618) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.01, loss=exponential, n_estimators=10;, score=(train=0.617, test=0.615) total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.01, loss=exponential, n_estimators=100;, score=(train=0.836, test=0.820) total time=   0.3s\n",
      "[CV 2/5] END learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.914, test=0.809) total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.895, test=0.837) total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=0.881, test=0.837) total time=   0.3s\n",
      "[CV 4/5] END learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=0.900, test=0.798) total time=   0.3s\n",
      "[CV 4/5] END learning_rate=0.1, loss=exponential, n_estimators=1000;, score=(train=0.979, test=0.775) total time=   2.4s\n",
      "[CV 5/5] END learning_rate=0.1, loss=exponential, n_estimators=1000;, score=(train=0.978, test=0.820) total time=   2.3s\n",
      "[CV 3/5] END learning_rate=100, loss=exponential, n_estimators=10;, score=(train=0.582, test=0.556) total time=   0.0s\n",
      "[CV 5/5] END learning_rate=100, loss=exponential, n_estimators=10;, score=(train=0.217, test=0.197) total time=   0.0s\n",
      "[CV 3/5] END learning_rate=100, loss=exponential, n_estimators=100;, score=(train=0.341, test=0.315) total time=   0.2s\n",
      "[CV 5/5] END learning_rate=100, loss=exponential, n_estimators=100;, score=(train=0.217, test=0.197) total time=   0.2s\n",
      "[CV 3/5] END learning_rate=100, loss=exponential, n_estimators=1000;, score=(train=0.575, test=0.539) total time=   1.5s\n",
      "[CV 1/5] END learning_rate=0.001, loss=log_loss, n_estimators=1000;, score=(train=0.833, test=0.810) total time=   2.1s\n",
      "[CV 1/5] END learning_rate=0.1, loss=log_loss, n_estimators=10;, score=(train=0.831, test=0.810) total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, loss=log_loss, n_estimators=10;, score=(train=0.832, test=0.815) total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, loss=log_loss, n_estimators=10;, score=(train=0.839, test=0.826) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, loss=log_loss, n_estimators=10;, score=(train=0.844, test=0.787) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.913, test=0.809) total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.905, test=0.831) total time=   0.2s\n",
      "[CV 2/5] END learning_rate=1, loss=log_loss, n_estimators=100;, score=(train=0.987, test=0.781) total time=   0.2s\n",
      "[CV 3/5] END learning_rate=1, loss=log_loss, n_estimators=100;, score=(train=0.985, test=0.820) total time=   0.2s\n",
      "[CV 2/5] END learning_rate=1, loss=exponential, n_estimators=10;, score=(train=0.879, test=0.809) total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1, loss=exponential, n_estimators=10;, score=(train=0.877, test=0.831) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=1, loss=exponential, n_estimators=100;, score=(train=0.983, test=0.788) total time=   0.2s\n",
      "[CV 2/5] END learning_rate=1, loss=exponential, n_estimators=100;, score=(train=0.985, test=0.820) total time=   0.2s\n",
      "[CV 2/5] END learning_rate=1, loss=exponential, n_estimators=1000;, score=(train=0.987, test=0.775) total time=   2.2s\n",
      "[CV 3/5] END learning_rate=1, loss=exponential, n_estimators=1000;, score=(train=0.985, test=0.843) total time=   1.9s\n",
      "[CV 5/5] END class_weight=balanced, criterion=log_loss, max_depth=100, splitter=best;, score=(train=0.982, test=0.775) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=log_loss, max_depth=100, splitter=random;, score=(train=0.985, test=0.760) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=log_loss, max_depth=None, splitter=best;, score=(train=0.982, test=0.781) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=log_loss, max_depth=None, splitter=random;, score=(train=0.985, test=0.737) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=gini, max_depth=None, splitter=best;, score=(train=0.985, test=0.792) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=gini, max_depth=None, splitter=best;, score=(train=0.985, test=0.742) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=gini, max_depth=None, splitter=best;, score=(train=0.982, test=0.820) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=gini, max_depth=None, splitter=random;, score=(train=0.985, test=0.704) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=log_loss, max_depth=1000, splitter=best;, score=(train=0.985, test=0.764) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=log_loss, max_depth=None, splitter=best;, score=(train=0.985, test=0.798) total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=None, n_estimators=100;, score=(train=0.982, test=0.837) total time=   0.4s\n",
      "[CV 5/5] END criterion=entropy, max_depth=1000, n_estimators=1000;, score=(train=0.982, test=0.843) total time=   3.9s\n",
      "[CV 1/5] END criterion=entropy, max_depth=None, n_estimators=10;, score=(train=0.965, test=0.777) total time=   0.0s\n",
      "[CV 1/5] END criterion=log_loss, max_depth=None, n_estimators=1000;, score=(train=0.985, test=0.782) total time=   3.6s\n",
      "[CV 5/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=lbfgs;, score=(train=0.788, test=0.787) total time=   0.1s\n",
      "[CV 4/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=newton-cholesky;, score=(train=0.802, test=0.775) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=newton-cholesky;, score=(train=0.787, test=0.787) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=sag;, score=(train=0.778, test=0.803) total time=   0.8s\n",
      "[CV 1/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=saga;, score=(train=0.792, test=0.743) total time=   1.1s\n",
      "[CV 2/5] END learning_rate=0.001, loss=exponential, n_estimators=10;, score=(train=0.616, test=0.618) total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.001, loss=exponential, n_estimators=100;, score=(train=0.616, test=0.618) total time=   0.3s\n",
      "[CV 4/5] END learning_rate=0.01, loss=log_loss, n_estimators=100;, score=(train=0.844, test=0.787) total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.01, loss=log_loss, n_estimators=100;, score=(train=0.836, test=0.848) total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.01, loss=exponential, n_estimators=100;, score=(train=0.836, test=0.820) total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.01, loss=exponential, n_estimators=100;, score=(train=0.842, test=0.781) total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.01, loss=exponential, n_estimators=1000;, score=(train=0.896, test=0.787) total time=   2.3s\n",
      "[CV 5/5] END learning_rate=0.01, loss=exponential, n_estimators=1000;, score=(train=0.896, test=0.854) total time=   2.2s\n",
      "[CV 3/5] END learning_rate=10, loss=exponential, n_estimators=10;, score=(train=0.387, test=0.348) total time=   0.0s\n",
      "[CV 5/5] END learning_rate=10, loss=exponential, n_estimators=10;, score=(train=0.735, test=0.770) total time=   0.0s\n",
      "[CV 2/5] END learning_rate=10, loss=exponential, n_estimators=100;, score=(train=0.463, test=0.416) total time=   0.2s\n",
      "[CV 4/5] END learning_rate=10, loss=exponential, n_estimators=100;, score=(train=0.205, test=0.247) total time=   0.2s\n",
      "[CV 2/5] END learning_rate=10, loss=exponential, n_estimators=1000;, score=(train=0.506, test=0.461) total time=   1.9s\n",
      "[CV 3/5] END class_weight=balanced, criterion=gini, max_depth=100, splitter=best;, score=(train=0.985, test=0.831) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=entropy, max_depth=100, splitter=best;, score=(train=0.982, test=0.775) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=entropy, max_depth=100, splitter=random;, score=(train=0.985, test=0.809) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=entropy, max_depth=1000, splitter=best;, score=(train=0.985, test=0.743) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=entropy, max_depth=1000, splitter=best;, score=(train=0.985, test=0.742) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=entropy, max_depth=1000, splitter=random;, score=(train=0.985, test=0.792) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=entropy, max_depth=None, splitter=best;, score=(train=0.987, test=0.770) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=entropy, max_depth=None, splitter=random;, score=(train=0.985, test=0.687) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=entropy, max_depth=None, splitter=random;, score=(train=0.982, test=0.798) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=log_loss, max_depth=100, splitter=best;, score=(train=0.985, test=0.753) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=log_loss, max_depth=100, splitter=random;, score=(train=0.985, test=0.758) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=log_loss, max_depth=1000, splitter=best;, score=(train=0.985, test=0.764) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=log_loss, max_depth=1000, splitter=random;, score=(train=0.985, test=0.787) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=log_loss, max_depth=None, splitter=best;, score=(train=0.982, test=0.787) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=gini, max_depth=100, splitter=best;, score=(train=0.987, test=0.770) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=gini, max_depth=100, splitter=random;, score=(train=0.985, test=0.753) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=gini, max_depth=1000, splitter=best;, score=(train=0.982, test=0.826) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=gini, max_depth=None, splitter=best;, score=(train=0.987, test=0.770) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=entropy, max_depth=100, splitter=best;, score=(train=0.985, test=0.775) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=entropy, max_depth=100, splitter=best;, score=(train=0.985, test=0.764) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=entropy, max_depth=None, splitter=best;, score=(train=0.985, test=0.760) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=entropy, max_depth=None, splitter=best;, score=(train=0.987, test=0.764) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=log_loss, max_depth=100, splitter=random;, score=(train=0.985, test=0.758) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=log_loss, max_depth=100, splitter=random;, score=(train=0.982, test=0.837) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=log_loss, max_depth=None, splitter=random;, score=(train=0.987, test=0.730) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=gini, max_depth=1000, splitter=random;, score=(train=0.985, test=0.758) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=entropy, max_depth=None, splitter=best;, score=(train=0.985, test=0.826) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=entropy, max_depth=None, splitter=best;, score=(train=0.985, test=0.753) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=log_loss, max_depth=1000, splitter=best;, score=(train=0.985, test=0.809) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=log_loss, max_depth=1000, splitter=best;, score=(train=0.985, test=0.730) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=gini, max_depth=1000, splitter=best;, score=(train=0.985, test=0.737) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=gini, max_depth=1000, splitter=best;, score=(train=0.987, test=0.770) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=gini, max_depth=1000, splitter=best;, score=(train=0.985, test=0.820) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=gini, max_depth=1000, splitter=best;, score=(train=0.985, test=0.742) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=log_loss, max_depth=1000, splitter=best;, score=(train=0.987, test=0.798) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=log_loss, max_depth=1000, splitter=random;, score=(train=0.985, test=0.764) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=log_loss, max_depth=None, splitter=random;, score=(train=0.982, test=0.781) total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=1000, n_estimators=1000;, score=(train=0.987, test=0.815) total time=   3.7s\n",
      "[CV 2/5] END criterion=entropy, max_depth=None, n_estimators=1000;, score=(train=0.987, test=0.809) total time=   3.6s\n",
      "[CV 4/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=newton-cholesky;, score=(train=0.798, test=0.775) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=saga;, score=(train=0.788, test=0.749) total time=   1.1s\n",
      "[CV 5/5] END class_weight=None, max_iter=10000, penalty=None, solver=newton-cg;, score=(train=0.802, test=0.820) total time=   0.1s\n",
      "[CV 1/5] END class_weight=None, max_iter=10000, penalty=None, solver=newton-cholesky;, score=(train=0.806, test=0.782) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, max_iter=10000, penalty=None, solver=newton-cholesky;, score=(train=0.797, test=0.781) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, max_iter=10000, penalty=None, solver=newton-cholesky;, score=(train=0.799, test=0.775) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, max_iter=10000, penalty=None, solver=newton-cholesky;, score=(train=0.806, test=0.770) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, max_iter=10000, penalty=None, solver=newton-cholesky;, score=(train=0.802, test=0.820) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, max_iter=10000, penalty=None, solver=sag;, score=(train=0.791, test=0.760) total time=   0.8s\n",
      "[CV 2/5] END class_weight=None, max_iter=10000, penalty=None, solver=sag;, score=(train=0.787, test=0.792) total time=   0.5s\n",
      "[CV 2/5] END learning_rate=0.01, loss=log_loss, n_estimators=1000;, score=(train=0.910, test=0.837) total time=   2.4s\n",
      "[CV 3/5] END learning_rate=1, loss=log_loss, n_estimators=10;, score=(train=0.891, test=0.848) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=1, loss=log_loss, n_estimators=10;, score=(train=0.892, test=0.792) total time=   0.0s\n",
      "[CV 3/5] END learning_rate=1, loss=log_loss, n_estimators=1000;, score=(train=0.985, test=0.837) total time=   2.3s\n",
      "[CV 4/5] END learning_rate=1, loss=log_loss, n_estimators=1000;, score=(train=0.985, test=0.792) total time=   2.3s\n",
      "[CV 5/5] END learning_rate=100, loss=exponential, n_estimators=1000;, score=(train=0.217, test=0.197) total time=   1.5s\n",
      "[CV 2/5] END learning_rate=0.001, loss=log_loss, n_estimators=1000;, score=(train=0.832, test=0.815) total time=   2.2s\n",
      "[CV 5/5] END learning_rate=0.1, loss=log_loss, n_estimators=10;, score=(train=0.835, test=0.848) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.903, test=0.804) total time=   0.2s\n",
      "[CV 3/5] END learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=0.881, test=0.837) total time=   0.2s\n",
      "[CV 4/5] END learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=0.900, test=0.798) total time=   0.2s\n",
      "[CV 3/5] END learning_rate=1, loss=log_loss, n_estimators=1000;, score=(train=0.985, test=0.803) total time=   2.2s\n",
      "[CV 4/5] END learning_rate=1, loss=log_loss, n_estimators=1000;, score=(train=0.985, test=0.792) total time=   2.2s\n",
      "[CV 3/5] END class_weight=balanced, criterion=log_loss, max_depth=None, splitter=best;, score=(train=0.985, test=0.831) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=log_loss, max_depth=None, splitter=best;, score=(train=0.985, test=0.730) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=entropy, max_depth=100, splitter=random;, score=(train=0.985, test=0.781) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=entropy, max_depth=100, splitter=random;, score=(train=0.982, test=0.758) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=entropy, max_depth=1000, splitter=best;, score=(train=0.985, test=0.726) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=entropy, max_depth=1000, splitter=best;, score=(train=0.987, test=0.758) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=log_loss, max_depth=1000, splitter=random;, score=(train=0.987, test=0.758) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=log_loss, max_depth=None, splitter=random;, score=(train=0.987, test=0.781) total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=1000, n_estimators=100;, score=(train=0.985, test=0.771) total time=   0.3s\n",
      "[CV 3/5] END criterion=gini, max_depth=None, n_estimators=1000;, score=(train=0.985, test=0.854) total time=   3.7s\n",
      "[CV 3/5] END criterion=log_loss, max_depth=1000, n_estimators=1000;, score=(train=0.985, test=0.848) total time=   3.7s\n",
      "[CV 5/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=newton-cg;, score=(train=0.788, test=0.787) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=saga;, score=(train=0.785, test=0.809) total time=   1.0s\n",
      "[CV 1/5] END class_weight=None, max_iter=10000, penalty=None, solver=newton-cg;, score=(train=0.806, test=0.782) total time=   0.1s\n",
      "[CV 2/5] END class_weight=None, max_iter=10000, penalty=None, solver=newton-cg;, score=(train=0.797, test=0.781) total time=   0.1s\n",
      "[CV 3/5] END class_weight=None, max_iter=10000, penalty=None, solver=sag;, score=(train=0.804, test=0.781) total time=   0.8s\n",
      "[CV 4/5] END class_weight=None, max_iter=10000, penalty=None, solver=sag;, score=(train=0.798, test=0.764) total time=   0.5s\n",
      "[CV 4/5] END learning_rate=0.001, loss=exponential, n_estimators=100;, score=(train=0.616, test=0.618) total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.01, loss=log_loss, n_estimators=10;, score=(train=0.617, test=0.612) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.01, loss=log_loss, n_estimators=100;, score=(train=0.833, test=0.810) total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.01, loss=exponential, n_estimators=10;, score=(train=0.616, test=0.618) total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.01, loss=exponential, n_estimators=10;, score=(train=0.616, test=0.618) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.01, loss=exponential, n_estimators=10;, score=(train=0.616, test=0.618) total time=   0.0s\n",
      "[CV 5/5] END learning_rate=0.01, loss=exponential, n_estimators=10;, score=(train=0.617, test=0.612) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.01, loss=exponential, n_estimators=100;, score=(train=0.831, test=0.810) total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.01, loss=exponential, n_estimators=100;, score=(train=0.832, test=0.815) total time=   0.2s\n",
      "[CV 2/5] END learning_rate=0.01, loss=exponential, n_estimators=1000;, score=(train=0.902, test=0.809) total time=   2.3s\n",
      "[CV 3/5] END learning_rate=0.01, loss=exponential, n_estimators=1000;, score=(train=0.879, test=0.843) total time=   2.3s\n",
      "[CV 1/5] END learning_rate=10, loss=exponential, n_estimators=10;, score=(train=0.247, test=0.212) total time=   0.0s\n",
      "[CV 2/5] END learning_rate=10, loss=exponential, n_estimators=10;, score=(train=0.463, test=0.416) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=10, loss=exponential, n_estimators=10;, score=(train=0.205, test=0.247) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=10, loss=exponential, n_estimators=100;, score=(train=0.256, test=0.218) total time=   0.2s\n",
      "[CV 5/5] END learning_rate=10, loss=exponential, n_estimators=100;, score=(train=0.735, test=0.775) total time=   0.2s\n",
      "[CV 3/5] END learning_rate=10, loss=exponential, n_estimators=1000;, score=(train=0.359, test=0.320) total time=   1.9s\n",
      "[CV 4/5] END class_weight=balanced, criterion=gini, max_depth=100, splitter=random;, score=(train=0.985, test=0.770) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=entropy, max_depth=None, splitter=random;, score=(train=0.985, test=0.764) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=entropy, max_depth=None, splitter=random;, score=(train=0.982, test=0.770) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=log_loss, max_depth=1000, splitter=random;, score=(train=0.987, test=0.747) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=log_loss, max_depth=1000, splitter=random;, score=(train=0.985, test=0.831) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=gini, max_depth=100, splitter=best;, score=(train=0.985, test=0.726) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=gini, max_depth=100, splitter=random;, score=(train=0.985, test=0.726) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=gini, max_depth=100, splitter=random;, score=(train=0.982, test=0.730) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=gini, max_depth=1000, splitter=random;, score=(train=0.985, test=0.775) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=gini, max_depth=None, splitter=random;, score=(train=0.985, test=0.803) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=entropy, max_depth=100, splitter=best;, score=(train=0.982, test=0.787) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=entropy, max_depth=100, splitter=random;, score=(train=0.985, test=0.743) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=log_loss, max_depth=100, splitter=best;, score=(train=0.985, test=0.831) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=log_loss, max_depth=100, splitter=best;, score=(train=0.985, test=0.730) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=log_loss, max_depth=None, splitter=random;, score=(train=0.987, test=0.758) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=log_loss, max_depth=None, splitter=random;, score=(train=0.985, test=0.815) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=entropy, max_depth=100, splitter=best;, score=(train=0.982, test=0.787) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=entropy, max_depth=100, splitter=random;, score=(train=0.985, test=0.765) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=entropy, max_depth=100, splitter=random;, score=(train=0.987, test=0.803) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=entropy, max_depth=100, splitter=random;, score=(train=0.985, test=0.843) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=log_loss, max_depth=None, splitter=best;, score=(train=0.985, test=0.732) total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=1000, n_estimators=10;, score=(train=0.973, test=0.760) total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=None, n_estimators=10;, score=(train=0.971, test=0.803) total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=None, n_estimators=10;, score=(train=0.964, test=0.770) total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=None, n_estimators=100;, score=(train=0.987, test=0.809) total time=   0.3s\n",
      "[CV 4/5] END criterion=gini, max_depth=None, n_estimators=1000;, score=(train=0.985, test=0.764) total time=   3.7s\n",
      "[CV 3/5] END criterion=log_loss, max_depth=1000, n_estimators=10;, score=(train=0.961, test=0.848) total time=   0.0s\n",
      "[CV 4/5] END criterion=log_loss, max_depth=1000, n_estimators=100;, score=(train=0.985, test=0.770) total time=   0.4s\n",
      "[CV 1/5] END criterion=log_loss, max_depth=None, n_estimators=10;, score=(train=0.961, test=0.777) total time=   0.1s\n",
      "[CV 3/5] END criterion=log_loss, max_depth=None, n_estimators=10;, score=(train=0.964, test=0.843) total time=   0.0s\n",
      "[CV 5/5] END criterion=log_loss, max_depth=None, n_estimators=10;, score=(train=0.969, test=0.820) total time=   0.1s\n",
      "[CV 3/5] END criterion=log_loss, max_depth=None, n_estimators=100;, score=(train=0.985, test=0.843) total time=   0.4s\n",
      "[CV 4/5] END criterion=log_loss, max_depth=None, n_estimators=1000;, score=(train=0.985, test=0.770) total time=   3.3s\n",
      "[CV 4/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=newton-cg;, score=(train=0.798, test=0.775) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=saga;, score=(train=0.785, test=0.809) total time=   1.1s\n",
      "[CV 1/5] END class_weight=None, max_iter=10000, penalty=None, solver=newton-cg;, score=(train=0.806, test=0.782) total time=   0.1s\n",
      "[CV 2/5] END class_weight=None, max_iter=10000, penalty=None, solver=newton-cg;, score=(train=0.797, test=0.781) total time=   0.1s\n",
      "[CV 5/5] END class_weight=None, max_iter=10000, penalty=None, solver=sag;, score=(train=0.788, test=0.809) total time=   0.8s\n",
      "[CV 1/5] END class_weight=None, max_iter=10000, penalty=None, solver=saga;, score=(train=0.799, test=0.754) total time=   0.7s\n",
      "[CV 4/5] END learning_rate=0.01, loss=log_loss, n_estimators=10;, score=(train=0.616, test=0.618) total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.01, loss=exponential, n_estimators=10;, score=(train=0.616, test=0.618) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.01, loss=exponential, n_estimators=100;, score=(train=0.842, test=0.781) total time=   0.3s\n",
      "[CV 3/5] END learning_rate=0.1, loss=log_loss, n_estimators=1000;, score=(train=0.983, test=0.826) total time=   2.4s\n",
      "[CV 4/5] END learning_rate=0.1, loss=log_loss, n_estimators=1000;, score=(train=0.980, test=0.787) total time=   2.3s\n",
      "[CV 2/5] END learning_rate=100, loss=log_loss, n_estimators=100;, score=(train=0.309, test=0.326) total time=   0.2s\n",
      "[CV 3/5] END learning_rate=100, loss=log_loss, n_estimators=100;, score=(train=0.398, test=0.382) total time=   0.2s\n",
      "[CV 3/5] END learning_rate=100, loss=log_loss, n_estimators=1000;, score=(train=0.398, test=0.382) total time=   2.3s\n",
      "[CV 5/5] END learning_rate=0.001, loss=exponential, n_estimators=1000;, score=(train=0.835, test=0.848) total time=   2.3s\n",
      "[CV 5/5] END learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=0.892, test=0.854) total time=   0.2s\n",
      "[CV 1/5] END learning_rate=0.1, loss=exponential, n_estimators=1000;, score=(train=0.982, test=0.777) total time=   2.2s\n",
      "[CV 5/5] END learning_rate=10, loss=log_loss, n_estimators=100;, score=(train=0.631, test=0.646) total time=   0.2s\n",
      "[CV 3/5] END learning_rate=10, loss=log_loss, n_estimators=1000;, score=(train=0.613, test=0.601) total time=   2.2s\n",
      "[CV 1/5] END class_weight=balanced, criterion=gini, max_depth=None, splitter=best;, score=(train=0.985, test=0.704) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=gini, max_depth=None, splitter=random;, score=(train=0.985, test=0.725) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=entropy, max_depth=100, splitter=random;, score=(train=0.985, test=0.758) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=entropy, max_depth=100, splitter=random;, score=(train=0.982, test=0.753) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=log_loss, max_depth=1000, splitter=random;, score=(train=0.985, test=0.747) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=log_loss, max_depth=1000, splitter=random;, score=(train=0.982, test=0.781) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=gini, max_depth=1000, splitter=best;, score=(train=0.985, test=0.732) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=gini, max_depth=1000, splitter=best;, score=(train=0.987, test=0.803) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=gini, max_depth=1000, splitter=best;, score=(train=0.985, test=0.803) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=gini, max_depth=1000, splitter=best;, score=(train=0.985, test=0.736) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=log_loss, max_depth=100, splitter=best;, score=(train=0.985, test=0.775) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=log_loss, max_depth=100, splitter=best;, score=(train=0.985, test=0.764) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=log_loss, max_depth=100, splitter=best;, score=(train=0.982, test=0.781) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=log_loss, max_depth=100, splitter=random;, score=(train=0.985, test=0.749) total time=   0.0s\n",
      "[CV 5/5] END criterion=gini, max_depth=None, n_estimators=10;, score=(train=0.961, test=0.820) total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=None, n_estimators=1000;, score=(train=0.985, test=0.782) total time=   3.7s\n",
      "[CV 1/5] END criterion=log_loss, max_depth=1000, n_estimators=1000;, score=(train=0.985, test=0.804) total time=   3.7s\n",
      "[CV 1/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=newton-cg;, score=(train=0.805, test=0.765) total time=   0.1s\n",
      "[CV 3/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=sag;, score=(train=0.788, test=0.787) total time=   0.8s\n",
      "[CV 4/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=sag;, score=(train=0.791, test=0.747) total time=   0.8s\n",
      "[CV 5/5] END class_weight=None, max_iter=10000, penalty=None, solver=saga;, score=(train=0.795, test=0.815) total time=   0.8s\n",
      "[CV 2/5] END learning_rate=0.001, loss=log_loss, n_estimators=1000;, score=(train=0.832, test=0.815) total time=   2.3s\n",
      "[CV 1/5] END learning_rate=0.1, loss=log_loss, n_estimators=10;, score=(train=0.831, test=0.810) total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.1, loss=log_loss, n_estimators=10;, score=(train=0.832, test=0.815) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.1, loss=log_loss, n_estimators=1000;, score=(train=0.982, test=0.771) total time=   2.2s\n",
      "[CV 2/5] END learning_rate=0.1, loss=log_loss, n_estimators=1000;, score=(train=0.985, test=0.820) total time=   2.2s\n",
      "[CV 4/5] END learning_rate=10, loss=exponential, n_estimators=1000;, score=(train=0.245, test=0.275) total time=   1.7s\n",
      "[CV 2/5] END class_weight=balanced, criterion=gini, max_depth=100, splitter=best;, score=(train=0.987, test=0.758) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=gini, max_depth=None, splitter=best;, score=(train=0.987, test=0.770) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=gini, max_depth=None, splitter=best;, score=(train=0.985, test=0.730) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=gini, max_depth=None, splitter=random;, score=(train=0.985, test=0.749) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=gini, max_depth=None, splitter=random;, score=(train=0.985, test=0.815) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=gini, max_depth=None, splitter=random;, score=(train=0.982, test=0.764) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=entropy, max_depth=100, splitter=best;, score=(train=0.987, test=0.775) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=entropy, max_depth=100, splitter=best;, score=(train=0.985, test=0.725) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=entropy, max_depth=100, splitter=random;, score=(train=0.987, test=0.725) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=entropy, max_depth=100, splitter=random;, score=(train=0.982, test=0.798) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=entropy, max_depth=1000, splitter=best;, score=(train=0.985, test=0.809) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=entropy, max_depth=1000, splitter=random;, score=(train=0.987, test=0.736) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=entropy, max_depth=None, splitter=best;, score=(train=0.985, test=0.709) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=entropy, max_depth=None, splitter=best;, score=(train=0.982, test=0.781) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=entropy, max_depth=None, splitter=random;, score=(train=0.985, test=0.775) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=log_loss, max_depth=100, splitter=best;, score=(train=0.985, test=0.815) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=log_loss, max_depth=100, splitter=random;, score=(train=0.985, test=0.798) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=log_loss, max_depth=1000, splitter=best;, score=(train=0.985, test=0.826) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=log_loss, max_depth=1000, splitter=random;, score=(train=0.985, test=0.775) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=log_loss, max_depth=None, splitter=best;, score=(train=0.985, test=0.753) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=gini, max_depth=100, splitter=best;, score=(train=0.985, test=0.787) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=gini, max_depth=100, splitter=random;, score=(train=0.982, test=0.775) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=gini, max_depth=1000, splitter=random;, score=(train=0.987, test=0.781) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=gini, max_depth=None, splitter=best;, score=(train=0.982, test=0.820) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=gini, max_depth=None, splitter=random;, score=(train=0.985, test=0.726) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=entropy, max_depth=100, splitter=random;, score=(train=0.985, test=0.770) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=entropy, max_depth=100, splitter=random;, score=(train=0.982, test=0.781) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=entropy, max_depth=None, splitter=best;, score=(train=0.982, test=0.787) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=entropy, max_depth=None, splitter=random;, score=(train=0.985, test=0.726) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=log_loss, max_depth=1000, splitter=best;, score=(train=0.985, test=0.781) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=log_loss, max_depth=1000, splitter=best;, score=(train=0.985, test=0.764) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=log_loss, max_depth=None, splitter=random;, score=(train=0.982, test=0.815) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=gini, max_depth=100, splitter=best;, score=(train=0.985, test=0.758) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=gini, max_depth=100, splitter=random;, score=(train=0.985, test=0.770) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=gini, max_depth=1000, splitter=random;, score=(train=0.987, test=0.764) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=gini, max_depth=None, splitter=random;, score=(train=0.987, test=0.753) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=entropy, max_depth=100, splitter=best;, score=(train=0.985, test=0.826) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=entropy, max_depth=100, splitter=best;, score=(train=0.985, test=0.753) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=entropy, max_depth=None, splitter=random;, score=(train=0.985, test=0.725) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=entropy, max_depth=None, splitter=random;, score=(train=0.982, test=0.798) total time=   0.0s\n",
      "[CV 1/5] END class_weight=balanced, criterion=log_loss, max_depth=1000, splitter=best;, score=(train=0.985, test=0.704) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=log_loss, max_depth=1000, splitter=best;, score=(train=0.987, test=0.764) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=gini, max_depth=1000, splitter=best;, score=(train=0.982, test=0.803) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=gini, max_depth=1000, splitter=random;, score=(train=0.985, test=0.721) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=gini, max_depth=1000, splitter=random;, score=(train=0.987, test=0.798) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=gini, max_depth=1000, splitter=random;, score=(train=0.985, test=0.792) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=log_loss, max_depth=1000, splitter=best;, score=(train=0.985, test=0.781) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=log_loss, max_depth=1000, splitter=random;, score=(train=0.982, test=0.747) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=log_loss, max_depth=None, splitter=random;, score=(train=0.985, test=0.770) total time=   0.0s\n",
      "[CV 1/5] END criterion=gini, max_depth=1000, n_estimators=1000;, score=(train=0.985, test=0.793) total time=   3.8s\n",
      "[CV 4/5] END criterion=entropy, max_depth=None, n_estimators=1000;, score=(train=0.985, test=0.775) total time=   3.7s\n",
      "[CV 3/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=lbfgs;, score=(train=0.795, test=0.787) total time=   0.1s\n",
      "[CV 3/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=lbfgs;, score=(train=0.794, test=0.787) total time=   0.2s\n",
      "[CV 1/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=sag;, score=(train=0.789, test=0.743) total time=   0.8s\n",
      "[CV 2/5] END class_weight=balanced, max_iter=10000, penalty=None, solver=sag;, score=(train=0.784, test=0.809) total time=   0.8s\n",
      "[CV 5/5] END class_weight=None, max_iter=10000, penalty=None, solver=saga;, score=(train=0.795, test=0.815) total time=   0.9s\n",
      "[CV 2/5] END learning_rate=0.01, loss=log_loss, n_estimators=10;, score=(train=0.616, test=0.618) total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.01, loss=exponential, n_estimators=10;, score=(train=0.616, test=0.618) total time=   0.0s\n",
      "[CV 2/5] END learning_rate=0.01, loss=exponential, n_estimators=100;, score=(train=0.832, test=0.815) total time=   0.3s\n",
      "[CV 4/5] END learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.913, test=0.809) total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.1, loss=log_loss, n_estimators=100;, score=(train=0.905, test=0.831) total time=   0.2s\n",
      "[CV 5/5] END learning_rate=0.1, loss=exponential, n_estimators=100;, score=(train=0.892, test=0.854) total time=   0.3s\n",
      "[CV 1/5] END learning_rate=0.1, loss=exponential, n_estimators=1000;, score=(train=0.982, test=0.777) total time=   2.5s\n",
      "[CV 5/5] END learning_rate=10, loss=exponential, n_estimators=100;, score=(train=0.735, test=0.770) total time=   0.2s\n",
      "[CV 1/5] END learning_rate=10, loss=exponential, n_estimators=1000;, score=(train=0.496, test=0.458) total time=   2.3s\n",
      "[CV 2/5] END learning_rate=100, loss=exponential, n_estimators=100;, score=(train=0.391, test=0.399) total time=   0.2s\n",
      "[CV 1/5] END learning_rate=100, loss=exponential, n_estimators=1000;, score=(train=0.228, test=0.285) total time=   1.7s\n",
      "[CV 4/5] END learning_rate=0.001, loss=log_loss, n_estimators=1000;, score=(train=0.844, test=0.787) total time=   2.3s\n",
      "[CV 3/5] END learning_rate=0.1, loss=log_loss, n_estimators=1000;, score=(train=0.983, test=0.826) total time=   2.2s\n",
      "[CV 4/5] END learning_rate=0.1, loss=log_loss, n_estimators=1000;, score=(train=0.980, test=0.787) total time=   2.2s\n",
      "[CV 5/5] END learning_rate=10, loss=exponential, n_estimators=1000;, score=(train=0.738, test=0.770) total time=   1.6s\n",
      "[CV 4/5] END class_weight=balanced, criterion=gini, max_depth=100, splitter=best;, score=(train=0.985, test=0.758) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=gini, max_depth=1000, splitter=best;, score=(train=0.985, test=0.736) total time=   0.0s\n",
      "[CV 2/5] END class_weight=balanced, criterion=gini, max_depth=None, splitter=best;, score=(train=0.987, test=0.775) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=gini, max_depth=None, splitter=random;, score=(train=0.982, test=0.775) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=entropy, max_depth=1000, splitter=best;, score=(train=0.985, test=0.820) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=entropy, max_depth=1000, splitter=best;, score=(train=0.985, test=0.730) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=entropy, max_depth=None, splitter=random;, score=(train=0.985, test=0.764) total time=   0.0s\n",
      "[CV 5/5] END class_weight=balanced, criterion=entropy, max_depth=None, splitter=random;, score=(train=0.982, test=0.747) total time=   0.0s\n",
      "[CV 3/5] END class_weight=balanced, criterion=log_loss, max_depth=1000, splitter=best;, score=(train=0.985, test=0.831) total time=   0.0s\n",
      "[CV 4/5] END class_weight=balanced, criterion=log_loss, max_depth=1000, splitter=best;, score=(train=0.985, test=0.764) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=gini, max_depth=100, splitter=best;, score=(train=0.985, test=0.754) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=gini, max_depth=100, splitter=best;, score=(train=0.987, test=0.792) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, criterion=gini, max_depth=None, splitter=random;, score=(train=0.987, test=0.781) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=gini, max_depth=None, splitter=random;, score=(train=0.985, test=0.792) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, criterion=gini, max_depth=None, splitter=random;, score=(train=0.985, test=0.775) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=gini, max_depth=None, splitter=random;, score=(train=0.982, test=0.753) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, criterion=log_loss, max_depth=1000, splitter=best;, score=(train=0.985, test=0.737) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, criterion=log_loss, max_depth=1000, splitter=random;, score=(train=0.985, test=0.815) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, criterion=log_loss, max_depth=None, splitter=random;, score=(train=0.982, test=0.781) total time=   0.0s\n",
      "[CV 2/5] END criterion=gini, max_depth=1000, n_estimators=10;, score=(train=0.971, test=0.826) total time=   0.0s\n",
      "[CV 4/5] END criterion=gini, max_depth=1000, n_estimators=100;, score=(train=0.985, test=0.770) total time=   0.3s\n",
      "[CV 5/5] END criterion=gini, max_depth=None, n_estimators=1000;, score=(train=0.982, test=0.843) total time=   3.8s\n",
      "[CV 5/5] END criterion=log_loss, max_depth=1000, n_estimators=1000;, score=(train=0.982, test=0.854) total time=   3.7s\n",
      "[CV 5/5] END class_weight=balanced, max_iter=10000, penalty=l2, solver=saga;, score=(train=0.780, test=0.803) total time=   1.1s\n",
      "[CV 5/5] END class_weight=None, max_iter=10000, penalty=None, solver=newton-cg;, score=(train=0.802, test=0.820) total time=   0.1s\n",
      "[CV 1/5] END class_weight=None, max_iter=10000, penalty=None, solver=newton-cholesky;, score=(train=0.806, test=0.782) total time=   0.0s\n",
      "[CV 2/5] END class_weight=None, max_iter=10000, penalty=None, solver=newton-cholesky;, score=(train=0.797, test=0.781) total time=   0.0s\n",
      "[CV 3/5] END class_weight=None, max_iter=10000, penalty=None, solver=newton-cholesky;, score=(train=0.799, test=0.775) total time=   0.0s\n",
      "[CV 4/5] END class_weight=None, max_iter=10000, penalty=None, solver=newton-cholesky;, score=(train=0.806, test=0.770) total time=   0.0s\n",
      "[CV 5/5] END class_weight=None, max_iter=10000, penalty=None, solver=newton-cholesky;, score=(train=0.802, test=0.820) total time=   0.0s\n",
      "[CV 1/5] END class_weight=None, max_iter=10000, penalty=None, solver=sag;, score=(train=0.791, test=0.760) total time=   0.8s\n",
      "[CV 2/5] END class_weight=None, max_iter=10000, penalty=None, solver=sag;, score=(train=0.787, test=0.792) total time=   0.6s\n",
      "[CV 5/5] END learning_rate=0.001, loss=log_loss, n_estimators=10;, score=(train=0.617, test=0.612) total time=   0.0s\n",
      "[CV 1/5] END learning_rate=0.001, loss=log_loss, n_estimators=1000;, score=(train=0.833, test=0.810) total time=   2.3s\n",
      "[CV 3/5] END learning_rate=0.1, loss=log_loss, n_estimators=10;, score=(train=0.839, test=0.826) total time=   0.0s\n",
      "[CV 4/5] END learning_rate=0.1, loss=log_loss, n_estimators=10;, score=(train=0.844, test=0.787) total time=   0.0s\n",
      "[CV 3/5] END learning_rate=0.1, loss=log_loss, n_estimators=1000;, score=(train=0.985, test=0.831) total time=   2.2s\n",
      "[CV 4/5] END learning_rate=0.1, loss=log_loss, n_estimators=1000;, score=(train=0.980, test=0.787) total time=   2.2s\n",
      "[CV 5/5] END learning_rate=10, loss=exponential, n_estimators=1000;, score=(train=0.738, test=0.775) total time=   1.7s\n"
     ]
    }
   ],
   "source": [
    "result.to_csv('submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e69bef7-1e67-4f80-a453-202da2991b17",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
